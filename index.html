<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"toulondu.github.io","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Sharing Knowledge And Learn More">
<meta property="og:type" content="website">
<meta property="og:title" content="Toulon&#39;s BLOG">
<meta property="og:url" content="https://toulondu.github.io/index.html">
<meta property="og:site_name" content="Toulon&#39;s BLOG">
<meta property="og:description" content="Sharing Knowledge And Learn More">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Toulon Du">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://toulondu.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Toulon's BLOG</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Toulon's BLOG</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Algorithm</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">机器学习中的各种优化器算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-16 00:20:14 / 修改时间：00:40:42" itemprop="dateCreated datePublished" datetime="2020-04-16T00:20:14+08:00">2020-04-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">基础算法</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>深度学习中的优化器有很多种，除了我们熟悉的梯度下降外，还有一些诸如 RMSProp，adam 等优秀的优化器。来了解一波~</p>
<h2 id="梯度下降-GD-BGD-MBGD"><a href="#梯度下降-GD-BGD-MBGD" class="headerlink" title="梯度下降 GD,BGD,MBGD"></a>梯度下降 GD,BGD,MBGD</h2><p>梯度下降（GD：gradient descent）大家都很熟悉，这里也不做详细介绍。整体就是先初始化求解参数，然后通过求解损失函数对求解参数的导数来对我们的求解参数进行更新，直到收敛。 </p>
<p>梯度下降分为BGD（batch：批量梯度下降），SGD（stochastic：随机梯度下降）和 MBGD(Mini-Batch：小批量梯度下降)，区别在于每次更新梯度时使用的样本的数量，分别为全部样本，单个样本和一部分样本。</p>
<p>梯度下降找到的最优解一般为函数的一个鞍点，即局部最优解。<br>MBGD和SGD因为样本较少，随机性太强，梯度往往震荡很大，如下：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/GDvsMBGD.png" class="" title="GDvsMBGD">

<p>要解决这个问题，使用MBGD进行优化时我们可以对学习率进行衰减来使之收敛。</p>
<p>PS:因为计算机本身的一些性质，将批次量设置为2的幂数计算会更快。</p>
<p>当然，还有比小批次下降更快的算法。但在学习它们之前，我们首先要了解指数加权平均。</p>
<h2 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h2><p>什么是指数加权平均？</p>
<p>参考吴恩达老师对此的讲解，用一个例子来进行说明：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/LondonTempreture.png" class="" title="LondonTempreture">

<p>上图是伦敦一年之中每天的温度情况，我们来对它做一些处理，把每天的温度值记作Vn。则：<br>V0=0， V1 = 0.9*V0 + 0.1*θ1， V2=0.9V1 + 0.1*θ2，…<br>θ为上图中当天真实的温度值，这个0.9我们记作β，那么公式记为：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/formulaVt.png" class="" title="formulaVt">

<p>稍微进行一下联想，这个V可以近似看做是之前1/(1-β)天的平均值，当我们分别取β=0.9(红色曲线)和0.98(绿色曲线)时，图像为：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/TempretureHandled.png" class="" title="TempretureHandled">

<p>β取0.9，我们把这个式子展开：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/UnfoldFormulaV.png" class="" title="UnfoldFormulaV">

<p>θ随着时间推后是β的指数级衰减，且一般来说，指数加权的衰减大约会在1/(1-β)后衰减到大约三分之一的程度，比如0.9的1/(1-0.9)次方约等于0.35，所以我们说它大约是最近10天的平均值。这就是指数加权平均名称的由来。</p>
<p>指数加权平均减少了存储空间的使用，当我们需要某个V的值时，只需要通过计算即可获得，因此它在机器学习中得到了大量的应用。</p>
<p>细心的你可能注意到，我们的V0取值为0，这会在计算初期的时候产生较大的误差值。<br>因此在很多时候，我们会对V的值进行<strong><em>偏差修正</em></strong>。<br>使 Vt = Vt/(1-β^t)  ，从而对Vt进行放大，而随着t的增大，放大率会逐渐趋于1。</p>
<p>有了这个基础，我们就可以介绍一些其它的优化器方法。</p>
<h2 id="动量梯度算法（momentum）"><a href="#动量梯度算法（momentum）" class="headerlink" title="动量梯度算法（momentum）"></a>动量梯度算法（momentum）</h2><p>算法的主要思想其实很简单，就是把我们刚才学习的指数加权平均用来计算梯度，然后用计算得到的梯度来进行参数更新。<br>在上面的梯度下降算法中，考虑多维度的情况，如下：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/momentumExample.png" class="" title="momentumExample">

<p>于收敛来说，竖直方向的震荡显然是无益的，我们希望能够减小竖直方向的震荡。</p>
<p>动量梯度下降的步骤如下<br>第t次迭代：</p>
<ol>
<li>用当前小批量样本 计算 参数W和 偏差b的导数</li>
<li>Vdw = βVdw + (1-β)dw， Vdb = βVdb + (1-β)db</li>
<li>W := W - αVdw,   b := b-αVdb </li>
</ol>
<p>end<br>我们还是对批次数量进行迭代，整个算法中中有2个超参数，α和β，α为学习率，β是我们上面学习到的指数加权。</p>
<p>为什么咋这么做可以有效呢？因为指数加权平均的平均，它就让垂直方向上相反的震荡被平均从而变小，而水平方向的震荡方向是相同的，平均值依然很大，故整体收敛速度就加快了。</p>
<p>PS:动量梯度下降时基本不用偏差修正，β基本都是选择0.9。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop全称为均方根传递(Root Mean Square prop), 它也可以加速梯度下降。<br>计算方式如下：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/RMSPropStep.png" class="" title="RMSPropStep">

<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/RMSPropImg.png" class="" title="RMSPropImg">
<p>RMSprop主要的思想是缩小大的震荡，加快小的震荡。如上图，垂直方向的震荡很大，而水平方向很小，RMSprop就会缩小垂直方向的震荡，加快水平方向的速度。<br>原理很简单，如果垂直方向的震荡较大，那么Sdb就会很大，那么作为除数，更新速度就减慢了，相反，水平方向较慢，Sdw就较小，W的更新速度就加快了。<br>当然，实际应用中，并没有垂直水平这么简单，我们加快的是慢的维度，减慢的是快的维度。<br>而且为了防止除0发生，通常在分母上我们会加一个很小的EPSON，大概10e-8</p>
<p>接下来，我们把RMSprop和动量结合起来，会得到一个更好的优化算法。<br>为了防止冲突，RMSprop中的β，我们用β_2表示</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>adaptive moment estimation 自适应矩估计<br>adam优化算法实际上就是将RMSprop和动量梯度下降结合起来的算法。<br>在机器学习领域和深度学习领域中，曾经提出了非常多的优化算法，但大多数算法都不能很好的适应于不同的网络结构，adam算法是少有的在非常多网络结构中都能够产生非常好效果的算法。</p>
<p>在机器学习领域和深度学习领域中，曾经提出了非常多的优化算法，比如Adagrad,Adadelta等，但大多数算法都不能很好的适应于不同的网络结构，adam算法是少有的在非常多网络结构中都能够产生非常好效果的算法。</p>
<p>初始化Vdw，Sdw,  Vdb, Sdb为0</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/AdamStep.png" class="" title="AdamStep">

<p>l为网络层数，两个β分别是动量梯度和RMSprop中的加权，α为学习率，同样，为了防止除0发生，通常在分母上我们会加一个很小的EPSON，大概10e-8（上图没加）。</p>
<p>几个超参数，一般β1选择0.9，β2选择0.999，EPSON选择10e-8。<br>而α，一个好的方法就是逐渐减小学习速率，使用一个衰减率来对学习率进行衰减。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/04/03/BERT%E7%AC%94%E8%AE%B0-%E7%AE%80%E4%BB%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/03/BERT%E7%AC%94%E8%AE%B0-%E7%AE%80%E4%BB%8B/" class="post-title-link" itemprop="url">BERT笔记-简介</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-03 13:48:42 / 修改时间：13:56:09" itemprop="dateCreated datePublished" datetime="2020-04-03T13:48:42+08:00">2020-04-03</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>BERT，全名 Bidirectional Encoder Representation from Transformers。是Google AI团队在2018年推出的一个NLP模型，在机器阅读理解顶级水平测试SQuAD1.1上全面超越人类表现，并在11项NLP测试中拿到最佳成绩。这也导致BERT的大火。</p>
<p>BERT的出现彻底改变了pre-train产生词向量和下游NLP训练任务间的关系。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">Transformer模型简介(笔记)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-02 22:04:42 / 修改时间：22:21:58" itemprop="dateCreated datePublished" datetime="2020-04-02T22:04:42+08:00">2020-04-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Transformer来自Google 2017年的一篇文章，在原来的Attention&amp;RNN模型上抛弃了RNN，用全attention的结构取得了更好的效果。<br>这里做一做自己学习的笔记，也算一个简单的介绍。<br>内容图片很多来自于原论文<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a>和<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>这篇文章。</p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>原论文给出的结构如下：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/structureOfTransformer.png" class="" title="transformer结构">
<p>可以看到由左右两个部分，左边的Encoders和右边的Decoders组成。两边都有一个”N×”,表示各自由N个同样的结构重复N次组成，原文中是6。就是下面图中的样子。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/encodersAndDecoders.png" class="" title="展开">

<h2 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h2><p>我们来看一看Encoder部分。</p>
<p>因为是NLP的案例，所以我们首先要把我们的输入数据，即词变成词向量，这通过embedding实现，embedding后的数据作为Encoder的输入。<br>虽然有很多encoder，但embedding只用在最下面一层的encoder上，其它的encoder都是用上一层encoder的输出作为输入。</p>
<p>每个encoder都是一样的结构，都由两个子结构组成：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/EachEncoder.png" class="" title="encoder组成">

<p>self-attention的作用是，当你在处理某个具体的词时，self-attention允许你从句子中的其它位置处寻找线索，从而对当前词的理解和预测起到帮助。</p>
<h3 id="self-attention-细节"><a href="#self-attention-细节" class="headerlink" title="self-attention 细节"></a>self-attention 细节</h3><p>计算self-attention主要有以下几个步骤</p>
<p><strong>第一步</strong><br>计算self-attention的第一步是从每个输入向量中创建出3个向量(Querry,Key,Value)。他们通过把embedding分别与三个矩阵相乘得到，三个矩阵通过训练过程得到。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/transformerSelfAttentionVectors.png" class="" title="QSV">

<p>Q,S,V较之embedding的维度要小，在文中的维度是64，而embedding的维度是512。不必完全一样，但这是一种使得计算比较稳定的结构选择。</p>
<ul>
<li>Q = WQ * x</li>
<li>K = WK * x</li>
<li>V = WV * x</li>
</ul>
<p><strong>第二步</strong><br>计算self-attention的第二步是计算一个score。 假设我们正在计算的句子的第一个单词为”Thinking”。我们需要把输入的句子中的每一个词与这个词运算来得到一个score，这个score决定了我们在encode当前词的时候句子其它位置所施加的影响。</p>
<p>计算方法是把当前次的Q与要计算的词的K值进行点乘。即如果我们要计算#1位置处的self-attention，第一个我们要计算的score将是把q1和k1点乘，第二个socre则是把q1和k2进行点乘。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/SelfAttentionScore.png" class="" title="计算score">


<p><strong>第三步和第四步</strong><br>第三步和第四步是将得到的score除以8，这个8是QKV向量的维度64的平方根。这可以让梯度更加稳定(直接归一值差距较大)。当然可以不是8，这里只是一个默认值。接着将结果传递给一个softmax操作，这将把socre的值标准化，使它们都为正，且和为1。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/selfAttentionSoftmax.png" class="" title="计算权重">

<p><strong>第五步</strong><br>第五步是将每个V与第四步的结果相乘。这一步从直觉上讲是保留当前词想要关注的其它词语的完整性，同时丢掉不相关的词语(通过乘以了非常小的数)。</p>
<p><strong>第六步</strong><br>将得到的带权重的数据向量相加。这将得到self-attention层在这个位置(我们这里是第一个词)的输出。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/selfAttentionOutput.png" class="" title="encoder输出">

<p>这就是self-attentionde的计算过程，结果向量我们将传递给接下来的 feed-forward nertal network处理。<br>当然，在实际实现中，这些计算都可以通过矩阵形式的计算从而更加快速。</p>
<p><strong>self-attention的矩阵计算</strong><br>使用矩阵，第二步到第六步实际上可以在一个公式内进行计算：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/selfAttentionMatrixCalculation.png" class="" title="矩阵计算公式">



<h3 id="multi-headed"><a href="#multi-headed" class="headerlink" title="multi-headed"></a>multi-headed</h3><p>文章进一步用一个叫做”multi-headed” attention的结构增强了self-attention。它从2个方面提升了attention层的表现：<br><strong>扩张了模型关注其他位置的能力</strong><br>在我们上面的例子中，对thinking的编码就包含了句中其它位置词的影响(当然，最大的影响依然是它自己)。在解析一些有明显指向性的代词时就显得非常有用。比如“The animal didn’t cross the street because it was too tired”中的”it”指代的谁。<br><strong>给了attention层多重”表述子空间”</strong><br>这主要通过多组[WQ,WK,QV]来实现，文中使用了8组WQ,WK,QV，这些矩阵都通过随机初始化赋值。即是说我们会得到8组QKV，从而得到8个输出矩阵。每一个都是输入数据的一个表述子空间。</p>
<p>在传递给feed-forward network前，我们需要将他们处理成一个矩阵。通过将这8个矩阵堆叠起来，再与一个权重矩阵WO相乘得到。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/transformer_attention_heads_weight_matrix_o.png" class="" title="concat结果">

<p>以上大概就是 multi-headed self-attention 的内容。原文将他们放到一张图上：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/encoderTotalLook.png" class="" title="整体造型">

<h3 id="position-encoding"><a href="#position-encoding" class="headerlink" title="position encoding"></a>position encoding</h3><p>因为放弃了使用RNN，那么句子中词与词的位置关系就被忽略了，文中使用了一种position encoding的方式将位置信息补入模型中。<br>这通过给每一个input embedding加上一个vector来实现。这些vector遵循一种<strong><em>特殊的模式</em></strong>，它存储了每个词的位置信息，通过把它与embedding相加，从而把这种信息代入到后面的QKV和点乘的计算过程中。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/transformer_positional_encoding_vectors.png" class="" title="位置编码">

<p>如果我们的embedding是512维的向量，那么要加的positional encoding 向量也是512维。</p>
<p>关于位置编码，文中使用的是三角函数的形式。</p>
<p>大概说一下什么是位置编码和为什么要使用三角函数。<br>要对位置进行编码，最简单的方式莫过于直接使用单词在文本中的位置，即1，2，3，…，N。但缺点过于明显，如果文本较长，那么位置编码的大小跨度就太大了，将这样的数据加入到模型训练中，很有可能是会喧宾夺主的抢占embedding的重要性。<br>同样，将刚才的顺序除以文本长度也是不行的，如1/N,2/N,3/N,…1。<br>我们需要位置信息，其中一个重要的信息就是相对位置信息，而这种处理方式，会导致相隔同样距离的两个词，在长度不同的文本中得到的相对位置信息不一致，甚至差距较大。<br>总结之后，那么真正适合用来做位置编码的函数似乎就是 连续且有界的周期性函数。有界保证值域不会太大，周期性保证一定程度上编码的差异会摆脱文本长度的影响，而连续则保证了两个比较靠近的词不会出现差距很大的情况。</p>
<p>于是文中使用了sin和cos函数，连续而且周期稳定，值域[-1,1]。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/PosEncodingformula.png" class="" title="位置编码公式">

<p>加入了dmodel和i两个参数，dmodel是embedding的维度，在文中就是512，用于增大位置编码的空间表现范围。i为向量的某一维度，dmodel=512，那么i就是[0,255],这样在奇偶维度分别使用sin和cos。这样就从取值范围和取值方法两个方向上增加了取值的多样性。让位置编码更加科学。</p>
<p>当然，这个函数作者应该也是通过自身的经验与不断的实验得到的。</p>
<p>PS：GOOGLE BERT中用了新的取位置信息的方法，position embedding，这是后话。</p>
<h3 id="残差网络-Residual-network-的使用"><a href="#残差网络-Residual-network-的使用" class="headerlink" title="残差网络(Residual network)的使用"></a>残差网络(Residual network)的使用</h3><p>另一个细节，就是哪里跑不掉的resNet的使用：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/resInEncoder.png" class="" title="resNet的使用">

<p>同样，在decoder中也使用到了resNet，如果是一个2个encoder和decoder的transformer，它长这样：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/resInTransformer.png" class="" title="resNet的使用2">

<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>介绍完Encoder，大多数Decoder里的组件的作用也明朗了。接下来看看他哥俩如何一起工作。</p>
<p>再贴一下模型结构图：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/structureOfTransformer.png" class="" title="transformer结构">
<p>可以看到Decoder中有一个 Encoder-Decoder Attention 层，它接受Encoder部分最后的输出作为计算attention的Key和Value，接受它下面的self-attention层的输出作为Query。</p>
<p>其次，Decoder部分的self-attention层也与Encoder中的不同，不同于Encoder中计算单词两两间的attention，Decoder中计算的是当前单词和它前面的单词的attention，同样，也要加入位置信息。</p>
<p>文章中有张非常形象的动图：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/transformerDecodingGif.gif" class="" title="transformer结构">

<p>注意在decoder中做self-attention的时候，当前输入只应该看到当前时刻以前的输出，比如在输出第二个词的时候，输入中是不应该出现第三个词的信息的。文中处理这种情况的方法是用了一个倒三角矩阵(第i行j列的元素表示第i个输入和第j个输入的attention)，将对角线右侧元素全部设置为负无穷，这样就防止了模型看到未来的信息。</p>
<h2 id="最后一层"><a href="#最后一层" class="headerlink" title="最后一层"></a>最后一层</h2><p>decoder将输出一堆floats组成的向量，将它转换成词语，就是最后一层的工作(通常是一个Linear+Softmax)。</p>
<p>Linear layer是一个简单的全连接层，将decoder的输出投射为一个比原来大很多的向量，叫做logits vector。</p>
<p>如果我们的词空间有10000个单词，那么10000就是这个logits vector的维度，向量中每个元素对应一个具体的词。接下来你就清楚了，softmax的作用是将这个logits vector的结果变成概率，概率最高的元素对应的词就是我们的输出。</p>
<h2 id="关于训练"><a href="#关于训练" class="headerlink" title="关于训练"></a>关于训练</h2><p>todo…</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>todo…</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a><br><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a><br><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">哈佛大学的pytorch版本源码</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">实现一个简单的人脸识别系统</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-25 15:27:57 / 修改时间：16:36:29" itemprop="dateCreated datePublished" datetime="2020-03-25T15:27:57+08:00">2020-03-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>下述的大多数方法来自 <strong><em><a href="https://arxiv.org/pdf/1503.03832.pdf" target="_blank" rel="noopener">FaceNet</a></em></strong>,也叫做<strong><em>DeepFace</em></strong>.</p>
<h2 id="脸部识别"><a href="#脸部识别" class="headerlink" title="脸部识别"></a>脸部识别</h2><p>一般来说，脸部识别问题可以分为两类：</p>
<ul>
<li>脸部验证(Face Verification)：”这是某个人吗？” 通过提供的输入的数据来识别是否是某个确定的人。比如机场系统扫描你的护照来确认你是否是正确的持有人，比如移动手机通过识别你的脸部确认你是拥有者从而解锁。总的来说，这是一个1对1匹配的问题。</li>
<li>脸部识别(Face Recognition)：”这是谁？” 通过提供的输入识别来识别对应的人是谁。比如公司的脸部识别打卡，通过识别脸部直接完成对应人员的打卡。总的来说，这是一个1对K的匹配问题。</li>
</ul>
<p>FaceNet 通过一个神经网络先将输入的脸部照片解码为一个128维向量，通过比较2个这样的向量，从而判断这两张图片是否是一个人。将输入数据与数据库中所有人员的照片进行对比，从而找到”这是谁”。</p>
<p>我们将用TensorFlow来实现这个程序：(tensorflow 1.X)</p>
<ul>
<li>1.实现三元损失函数</li>
<li>2.用一个预训练的模型来将脸部图片解码为128维的向量</li>
<li>3.使用这些代码来进行脸部验证和脸部识别</li>
</ul>
<p>另外，我们使用通道优先(channels-first)。 也就是说对于输入数据的维度表示，我们使用(m,nC,nH,nW), 而不是(m,nH,nW,nC)。当然，channels-first 和 channels-last都有各自的理由，至今社区也没有一个统一的标准。</p>
<h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><p>先导入包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate</span><br><span class="line">from keras.models import Model</span><br><span class="line">from keras.layers.normalization import BatchNormalization</span><br><span class="line">from keras.layers.pooling import MaxPooling2D, AveragePooling2D</span><br><span class="line">from keras.layers.merge import Concatenate</span><br><span class="line">from keras.layers.core import Lambda, Flatten, Dense</span><br><span class="line">from keras.initializers import glorot_uniform</span><br><span class="line">from keras.engine.topology import Layer</span><br><span class="line">from keras import backend as K</span><br><span class="line">K.set_image_data_format(&#39;channels_first&#39;)</span><br><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">from numpy import genfromtxt</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from fr_utils import *</span><br><span class="line">from inception_blocks_v2 import *</span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold&#x3D;np.nan)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="0-简陋的脸部识别"><a href="#0-简陋的脸部识别" class="headerlink" title="0.简陋的脸部识别"></a>0.简陋的脸部识别</h2><p>在脸部验证中，你需要确定给到的两张图片是否是一个人。最简单的方法就是直接将两张图片一个像素一个像素的进行比较。如果两张图片间的间距小于某个阈值，他们可能就是一个人。</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/LisaFaceReco.png" class="" title="逐像素比较">

<p>不难想到这个算法的表现会很差。因为光线的变化、人物脸部方向、甚至是头部位置的微小变化，都会导致像素值的改变。</p>
<p>与其使用原图片，我们可以使用编码后的图片数据。即f(img)。</p>
<p>将图片编码后的数据进行元素级的比较，我们可以得到一个更加准确的关于脸部验证的结果。</p>
<hr>
<h2 id="1-将脸部图片编码为128维的向量"><a href="#1-将脸部图片编码为128维的向量" class="headerlink" title="1.将脸部图片编码为128维的向量"></a>1.将脸部图片编码为128维的向量</h2><h3 id="1-1-使用卷积网络来进行编码"><a href="#1-1-使用卷积网络来进行编码" class="headerlink" title="1.1 使用卷积网络来进行编码"></a>1.1 使用卷积网络来进行编码</h3><p>FaceNet模型需要使用非常多的数据和很长的时间来进行训练。这里我们跳过这个步骤，直接载入别人已经训练好的权重。 网络结构采用了 <a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Szegedy等人</a>文中的inception模型。 我们使用一个已经实现好的inception network的实现(在inception_blocks_v2.py中，略)。</p>
<p>几个需要知道的知识点：</p>
<ul>
<li>这个网络采用96×96维度的RGB图像作为输入。特别地，输入一个脸部照片(或者多批次的m照片组)作为张量，形状为：(m,nC,nH,nW) = (m,3,96,96)</li>
<li>它将输出一个(m,128)的矩阵，即将每一张图片都编码为128维的向量。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FRmodel &#x3D; faceRecoModel(input_shape&#x3D;(3, 96, 96))</span><br><span class="line">print(&quot;Total Params:&quot;, FRmodel.count_params())</span><br></pre></td></tr></table></figure>
<p>输出：Total Params: 3743280</p>
<p>通过使用一个128元的全连接层作为它的最后一层，这就保证了模型将输出128维的向量。接着，使用这两个向量进行两张图片的比较：</p>


<p>如果编码符合以下判别标准，则是一个好的编码:</p>
<ul>
<li>对同一个人的不同图片的编码较为相似</li>
<li>对不同人的图片的编码差异较大</li>
</ul>
<p>三元损失函数将以上标准公式化了，并且试图将相同人的图片的编码缩小，将不同人图片的编码拉大。</p>
<h3 id="1-2-三元损失函数对于一张图片x，我们声明它的编码为f-x-f是由神经网络计算的方法。"><a href="#1-2-三元损失函数对于一张图片x，我们声明它的编码为f-x-f是由神经网络计算的方法。" class="headerlink" title="1.2 三元损失函数对于一张图片x，我们声明它的编码为f(x), f是由神经网络计算的方法。"></a>1.2 三元损失函数对于一张图片x，我们声明它的编码为f(x), f是由神经网络计算的方法。</h3><img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/FaceNetFx.png" class="" title="f(x)方法">

<p>三元损失函数的训练需要用到三元组数据，每个三元组包含三张图片(A,P,N)</p>
<ul>
<li>A 是一张锚图片 - 某人的头部图像</li>
<li>P 是一张”正”图片 - 与A图片中是同一个人物</li>
<li>N 是一张”负”图片 - 与A图片中不是同一个人物</li>
</ul>
<p>我们用(A(i),P(i),N(i))(都是上标)来声明第i个训练样本。<br>我们希望图A(i)与P(i)的距离至少比A(i)和N(i)的距离近至少一个α的值：</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/lossFormulaSingle.png" class="" title="单个样本的三元损失公式">
<p>那么总的损失函数就是：</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/lossFormulaTotal.png" class="" title="三元损失公式">
<p>[z]+表示 max(z,0)。表示一旦A与P距离和A与N距离达到我们的要求，损失就为0，否则它的值就是损失值。</p>
<p><strong><em>Notes:</em></strong></p>
<ul>
<li>公式中的第一个部分是锚图片A和正图片P的距离，你希望它尽可能的小</li>
<li>公式的第二个部分则是锚图片A和负图片N的距离，你希望它相对较大</li>
<li>α叫做边距(margin)，这是一个人为选择的超参数，我们使用 α = 0.2</li>
</ul>
<p>大多数的实现里会将编码后的向量进行一次L2归一化，这里我们不用担心~</p>
<p>实现上面公式中的三元损失函数，需要4个步骤：</p>
<ol>
<li>计算锚图片A和正图片P间的距离</li>
<li>计算锚图片A和负图片N间的距离</li>
<li>对每个三元组样本进行公式计算</li>
<li>将每组样本经步骤3得到的值与0取max并取和</li>
</ol>
<p>PS：</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/L2formula.png" class="" title="L2 Norm计算方法">

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def triplet_loss(y_true, y_pred, alpha &#x3D; 0.2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    三元损失函数的实现</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    y_true -- true 标签, 当你在keras中定义loss时需要, 在这个方法中你不需要它.</span><br><span class="line">    y_pred -- python list 包含三个对象:</span><br><span class="line">            anchor -- 锚图片编码后的结果, 形状为 (None, 128)</span><br><span class="line">            positive -- 正图片编码后的结果, 形状为(None, 128)</span><br><span class="line">            negative -- 负图片编码后的结果, 形状为 (None, 128)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    loss -- 数字, 损失值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    anchor, positive, negative &#x3D; y_pred[0], y_pred[1], y_pred[2]</span><br><span class="line">    </span><br><span class="line">    # Step 1</span><br><span class="line">    pos_dist &#x3D; tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),-1)</span><br><span class="line">    # Step 2</span><br><span class="line">    neg_dist &#x3D; tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),-1)</span><br><span class="line">    # Step 3</span><br><span class="line">    basic_loss &#x3D; tf.maximum(tf.add(tf.subtract(pos_dist,neg_dist),alpha),0)</span><br><span class="line">    # Step 4</span><br><span class="line">    loss &#x3D; tf.reduce_sum(basic_loss)</span><br><span class="line">    </span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-载入预训练模型"><a href="#2-载入预训练模型" class="headerlink" title="2.载入预训练模型"></a>2.载入预训练模型</h2><p>FaceNet通过最小化三元损失函数来进行训练。但训练需要大量的数据和计算时间，这里我们就不从头训练了。我们直接读取一个预训练的模型。用下面的代码读取来读取一个模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FRmodel.compile(optimizer &#x3D; &#39;adam&#39;, loss &#x3D; triplet_loss, metrics &#x3D; [&#39;accuracy&#39;])</span><br><span class="line">load_weights_from_FaceNet(FRmodel)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-应用模型"><a href="#3-应用模型" class="headerlink" title="3.应用模型"></a>3.应用模型</h2><p>假定我们构建的这个系统是一个门禁系统，用于给某公司利用脸部识别来确定是否允许某人进入公司建筑。</p>
<p>要通过门禁，每个人要先在入口处刷门禁卡，脸部识别系统会识别他们是否是他们所声明的人。</p>
<h3 id="3-1-脸部识别"><a href="#3-1-脸部识别" class="headerlink" title="3.1 脸部识别"></a>3.1 脸部识别</h3><p>我们先建立一个数据库，它存放了所有被允许进入建筑人员的编码后向量数据。它将用到img_to_encoding(image_path,model)方法，这个方法在输入图片数据上通过模型的前向传播来获得结果。</p>
<p>因为是教程，简便起见，我们直接用一个dict来充当数据库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">database &#x3D; &#123;&#125;</span><br><span class="line">database[&quot;danielle&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;danielle.png&quot;, FRmodel)</span><br><span class="line">database[&quot;younes&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;younes.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;tian&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;tian.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;andrew&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;andrew.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;kian&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;kian.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;dan&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;dan.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;sebastiano&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;sebastiano.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;bertrand&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;bertrand.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;kevin&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;kevin.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;felix&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;felix.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;benoit&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;benoit.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;arnaud&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;arnaud.jpg&quot;, FRmodel)</span><br></pre></td></tr></table></figure>

<p>接下来，当一个人走到前门处并刷卡，你就可以从数据库中查找他的编码，然后再进行脸部匹配，主要以下几个步骤：</p>
<ol>
<li>将前门摄像机捕捉的图片进行编码</li>
<li>计算上一步的编码与数据库中找到的对应id人员的编码间的间距</li>
<li>如果间距小于0.7，开门</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def verify(image_path, identity, database, model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    验证存放在image_path中的人是否和数据库identity对应的人是同一个</span><br><span class="line">    </span><br><span class="line">    参数:</span><br><span class="line">    image_path -- 图片地址</span><br><span class="line">    identity -- string, 要识别者的名字(来自于刷卡id). 必须是建筑进入允许的人员.</span><br><span class="line">    database -- python dictionary 数据字典 人名:头像编码 (向量).</span><br><span class="line">    model -- keras的 inception 模型</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    dist -- image_path存储的图像和identity对应的图像的间距</span><br><span class="line">    door_open -- True代表开门，False代表不开门</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Step 1:</span><br><span class="line">    encoding &#x3D; img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    # Step 2: </span><br><span class="line">    dist &#x3D; np.linalg.norm(encoding - database[identity])</span><br><span class="line">    # Step 3: </span><br><span class="line">    if dist &lt; 0.7:</span><br><span class="line">        print(&quot;It&#39;s &quot; + str(identity) + &quot;, welcome in!&quot;)</span><br><span class="line">        door_open &#x3D; True</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;It&#39;s not &quot; + str(identity) + &quot;, please go away&quot;)</span><br><span class="line">        door_open &#x3D; False</span><br><span class="line">        </span><br><span class="line">    return dist, door_open</span><br></pre></td></tr></table></figure>
<p>用了 <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html" target="_blank" rel="noopener">np.linalg.norm</a>来计算间距，不传递第二个参数即计算F-范数。</p>
<p>我们传入一张正确的图片试一试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">verify(&quot;images&#x2F;camera_0.jpg&quot;, &quot;younes&quot;, database, FRmodel)</span><br></pre></td></tr></table></figure>
<p>输出：It’s younes, welcome in!</p>
<p>再来一张错误的呢：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">verify(&quot;images&#x2F;camera_2.jpg&quot;, &quot;kian&quot;, database, FRmodel)</span><br></pre></td></tr></table></figure>
<p>输出：It’s not kian, please go away</p>
<h3 id="3-2-脸部识别"><a href="#3-2-脸部识别" class="headerlink" title="3.2 脸部识别"></a>3.2 脸部识别</h3><p>脸部认证系统基本完成了，但是如果系统内某人丢失了ID卡，他再次回到办公室就不能再进去了！(需要刷卡)</p>
<p>要解决这个问题，你就需要将系统改造成一个脸部识别系统。这样大家就都不需要带id卡了。一个被授权的人只要走到前门，门就会自动打开！</p>
<p>很简单，只需要一个遍历，直接上代码即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def who_is_it(image_path, database, model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    实现脸部识别系统，识别image_path图片人的身份</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    略</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    min_dist -- image_path图片与数据库中图片的最小间距</span><br><span class="line">    identity -- 最小间距对应的人</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    encoding &#x3D; img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # 初始化最小值，整大点</span><br><span class="line">    min_dist &#x3D; 100</span><br><span class="line">    </span><br><span class="line">    for (name, db_enc) in database.items():</span><br><span class="line">        </span><br><span class="line">        dist &#x3D; np.linalg.norm(encoding - db_enc)</span><br><span class="line"></span><br><span class="line">        if dist &lt; min_dist:</span><br><span class="line">            min_dist &#x3D; dist</span><br><span class="line">            identity &#x3D; name</span><br><span class="line"></span><br><span class="line">    if min_dist &gt; 0.7:</span><br><span class="line">        print(&quot;Not in the database.&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print (&quot;it&#39;s &quot; + str(identity) + &quot;, the distance is &quot; + str(min_dist))</span><br><span class="line">        </span><br><span class="line">    return min_dist, identity</span><br></pre></td></tr></table></figure>
<p>试一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">who_is_it(&quot;images&#x2F;camera_0.jpg&quot;, database, FRmodel)</span><br></pre></td></tr></table></figure>
<p>输出：it’s younes, the distance is 0.659393</p>
<p>激动人心的时候来了，我们把Lisa的图片裁剪成96×96再放入：</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/Lisa.png" class="" title="Lisa"> <img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/camera_Lisa.png" class="" title="LisaInCamera">
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">database[&quot;lisa&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;Lisa.png&quot;, FRmodel)</span><br><span class="line">who_is_it(&quot;images&#x2F;camera_Lisa.png&quot;, database, FRmodel)</span><br></pre></td></tr></table></figure>
<p>输出： it’s lisa, the distance is 0.597898<br>成功！！！</p>
<p>这样，一个简陋版本的脸部识别系统就完成啦！</p>
<h3 id="一些提升的方法"><a href="#一些提升的方法" class="headerlink" title="一些提升的方法"></a>一些提升的方法</h3><p>这里就不一一实现了，还有一些可以提升算法效果的方法：</p>
<ul>
<li>对于每个人，多在数据库中放几张照片，比如不同角度的，不同光线的，不同时间的。在刷脸时，将之与数据库中每个人的多张图片进行比较，这样可以提高模型准确度。</li>
<li>运用一个裁剪算法，将图片尽量剪到只剩下脸部。这样可以尽量排除不相关因素的干扰，也能提高准确度。</li>
</ul>
<hr>
<h2 id="引用："><a href="#引用：" class="headerlink" title="引用："></a>引用：</h2><ul>
<li>Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). <a href="https://arxiv.org/pdf/1503.03832.pdf" target="_blank" rel="noopener">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></li>
<li>Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf (2014). <a href="https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf" target="_blank" rel="noopener">DeepFace: Closing the gap to human-level performance in face verification</a></li>
<li>The pretrained model we use is inspired by Victor Sy Wang’s implementation and was loaded using his code: <a href="https://github.com/iwantooxxoox/Keras-OpenFace" target="_blank" rel="noopener">https://github.com/iwantooxxoox/Keras-OpenFace</a>.</li>
<li>Our implementation also took a lot of inspiration from the official FaceNet github repository: <a href="https://github.com/davidsandberg/facenet" target="_blank" rel="noopener">https://github.com/davidsandberg/facenet</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">卷积神经网络介绍</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-24 15:07:43" itemprop="dateCreated datePublished" datetime="2020-03-24T15:07:43+08:00">2020-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-26 21:22:36" itemprop="dateModified" datetime="2020-03-26T21:22:36+08:00">2020-03-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>传统的神经网络的全连接层(full connected)在遭遇较大的输入数据时，需要训练的参数将会非常多。举个例子，一张1000×1000的图片，作flatten处理后就有了1000×1000×3的输入维度，如果第二层的隐藏单元数是1000，参数W的维度就是3000000×1000。<br>参数过多，就意味着训练难度变高。在这种情况下，卷积神经网络就诞生了。</p>
<h2 id="什么是卷积"><a href="#什么是卷积" class="headerlink" title="什么是卷积"></a>什么是卷积</h2><p>我们先来看一下最基础的一点：什么是卷积？</p>
<p>用一个例子来进行说明，假如我们有一张6×6的图片，为了方便理解，我们暂时假定它第三个维度为1，即通道数为1。那么一次卷积操作如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/conv01.png" class="" title="卷积1">

<p>如上图所示，左一的6×6矩阵代表我们的输入矩阵，中间的3×3矩阵叫做过滤器(filter)，也叫做卷积核，而二者中间的”*“就表示<strong><em>卷积</em></strong>，它和计算机中的乘法符号一致。</p>
<p>卷积的运算方法就是 从输入矩阵的左上角开始，从左到右每一次取出一个与卷积核维度一样大小的矩阵，这里是3×3。 将取出的矩阵的每一位与卷积核对应为相乘并相加，得到的数据填在右侧结果矩阵的对应位置。</p>
<p>第二步如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/conv02.png" class="" title="卷积2">

<p>以此类推，往右不能再走时，就向下移动一行，再次从最左边开始取。<br>这样，最终我们可以得到一个4×4的结果矩阵(一张变小的图片)。</p>
<h3 id="卷积在做什么"><a href="#卷积在做什么" class="headerlink" title="卷积在做什么"></a>卷积在做什么</h3><p>你可能会感到困惑，这样的一步所谓的操作到底做了些什么。<br>就我们上面的例子来说，这样一个卷积核的效果可以用六个字来概括：“垂直边缘检测”。<br>如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/conv03.png" class="" title="卷积3">
<p>可以看到，结果图片的中部有一道明显的白色，这就是检测出来的垂直边缘。是的，它看起来与左边的输入图片有些不符合，显得比较厚，这是因为我们选择的图片较小(才6×6)，当把图片变大时，结果就变得较为可观了。</p>
<p>既然有垂直边缘检测的卷积核，自然就有其它功能的卷积核，比如水平边缘检测卷积核，比如着重突出某些边缘特征的卷积核等。</p>
<p>而卷积核的值是可以通过训练得到的，这点我们将在后面介绍。</p>
<h2 id="填充-padding"><a href="#填充-padding" class="headerlink" title="填充-padding"></a>填充-padding</h2><p>除了卷积，conv网络的第二个重要的积木就是<strong><em>填充</em></strong>。</p>
<p>在上一节介绍卷积时，一个6×6的图片经过一次卷积操作变成了4×4，可见卷积操作是会让输入数据的维度降低的。这样的操作多来几次，再大的图片也遭不住。因此，为了构建深层网络，填充就必不可少。</p>
<p>首先，关于卷积操作减小维度，是有一个通用的计算公式的：<br><strong><em>结果矩阵的维度 = (n-f+1) × (n-f+1)</em></strong><br>其中f为卷积核的维度，n为输入矩阵的维度。</p>
<p>除了会减小图片的维度外，卷积还会导致图片角落的像素只能被使用到一次，而中间的数据则会被使用多次，这样会导致图片边缘的数据不能得到足够的利用，甚至被丢失了。</p>
<p>所以所谓填充，就是填充数据的边缘：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/padding01.png" class="" title="填充1">
<p>通过在原图片边缘填充一层使之变成8×8的图像，从而经过卷积操作后，你依然可以得到一个6×6的图像。</p>
<p>一般来说，在卷积网络中大多数时候使用这样的填充，即使数据在填充后进行卷积得到与原数据同样维度的数据。这样的卷积操作我们称之为<strong><em>same卷积</em></strong>。</p>
<p>除此之外还有full卷积和valid卷积，前者让图像最角落的元素也可以被充分利用，而后者则是不做填充。</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/padding02.png" class="" title="full卷积">
<p>上图为full卷积</p>
<h2 id="步幅-stride-卷积"><a href="#步幅-stride-卷积" class="headerlink" title="步幅(stride)卷积"></a>步幅(stride)卷积</h2><p>在之前的介绍中，我们每次计算卷积后，都是在原图片上向右或者向下移动一步。而步幅卷积，顾名思义，就是将这个一步扩展开来，每次移动s步，我们以2为例：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/stride01.png" class="" title="步幅卷积1">

<p>如上为第一步和第二步，此外，当需要向下移动时，也是移动s步</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/stride03.png" class="" title="步幅卷积3">

<p>容易计算，经过步幅为s的卷积后，结果矩阵维度的计算方式为：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/strideFormula.png" class="" title="步幅卷积结果矩阵维度公式">

<p>假如当向右移动s步已经超出了原矩阵范围，即当不能被整除的时候，就直接舍弃剩余部分直接向下走了。即向下取整。</p>
<h2 id="三维数据的卷积"><a href="#三维数据的卷积" class="headerlink" title="三维数据的卷积"></a>三维数据的卷积</h2><p>对于现实情况中的图片来说，因为RGB格式的存在，我们的输入数据大多都是3维的，那么，如何将卷积扩展到三维数据上去呢？</p>
<p>首先，卷积核（过滤器）也要变成三维，且第三个维度要和原矩阵相同(第三个维度在卷积中我们一般叫做<strong><em>通道</em></strong>)，如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/convIn3D01.png" class="" title="三维数据卷积01">
<p>可以看到，最终的结果是一个4*4的矩阵，没有了第三个维度，计算方法其实很简单，基本就是二维计算方式的三维扩展：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/convIn3D02.png" class="" title="三维数据卷积02">
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/convIn3D03.png" class="" title="三维数据卷积03">
<p>即把每个3×3×3 = 27个格子的数字对应相乘并相加，然后右移和下移即可，因为深度一致，并不需要关心第三个维度。</p>
<p>卷积核的设计很灵活，假如你把卷积核的第二层和第三层都设置为0，相当于在结果矩阵中就只有你对第一层的卷积，在RGB图片中，你就只检测了红色通道的边缘。</p>
<p>更常用的是，你可以同时使用多个卷积核，然后把得到的结果堆叠，得到通道数大于1的结果：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/convIn3D04.png" class="" title="三维数据卷积04">
<p><strong>所以卷积结果矩阵的通道数，取决于卷积核的数量，记住这一点。</strong></p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>介绍了这么多，我们拉通来看一下，把上述知识点结合起来，就是卷积神经网络的一层：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/aLayerInConvNet.png" class="" title="卷及网络中的一层">
<p>之前我们说过，每一个过滤器(卷积核)都相当于是过滤出原数据的一个特性。上图中，用了2个过滤器，将每个过滤器得到的结果再进行一次激活。激活函数可以说是神经网络中的一种重要工具，一般是一个非线性函数，可以简单地理解为将数据中的特点放大并且保证了多层网络的实用性。激活之后再重叠在一起，就得到了我们最后4×4×2的结果。</p>
<p>如果你知道普通神经网络的结构，它每一层的运算就是先进行 z[l] = W*a[l-1] + b[l]，再进行 a[l] = g(z[l])。可以看到对于我们的卷积神经网络，这一层的操作，也可以对应成这个公式，过滤器就是我们的W。</p>
<p>既然过滤器是我们W，上面我们也说过过滤器的值可以训练，那么这么一层卷积网络，我们需要训练多少参数呢？<br>假设我们有10个3×3×3的过滤器，那么我们需要 3×3×3×10 + 10(b参数，偏差值) 个，即280个参数。</p>
<h3 id="ConvNet中的符号"><a href="#ConvNet中的符号" class="headerlink" title="ConvNet中的符号"></a>ConvNet中的符号</h3><p>在卷积网络中，你会看到很多各种符号以及参数、中间值，结果的维度，我们稍作总结：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/notaionInConvNet.png" class="" title="卷积网络中的符号">

<p>以上就是卷积网络中的一层了，那么我们如何扩展它来构建一个卷积网络呢？</p>
<p>PS: ConvNet是卷积神经网络的简写</p>
<h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><p>现在假如你有一些照片，你想通过卷积网络来实现图片的分类，假如输入图片的大小为39×39×3：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/aBriefConvNet.png" class="" title="一个简单的卷积网络结构">
<p>接着，我们可以把最后这个7×7×40的矩阵展开成1960的向量，接着我们可以使用普通神经网络的手段，使用逻辑回归或者一个softmax层来进行最后结果的计算。</p>
<p>从上面可以看出一个大多数卷积神经网络的趋势，即随着层数的增加，矩阵的宽高会逐渐减小，而通道数量会逐渐增加。</p>
<h3 id="其它的层类型"><a href="#其它的层类型" class="headerlink" title="其它的层类型"></a>其它的层类型</h3><p>上面的网络中，这些使用卷积核的层都可以叫做卷积层，但神经网络层中还有一些其他的层类型，比如池化(pooling)层，全连接(full connect/FC)层等。<br>将卷积层与这二者结合，我们可以搭建出更好的卷积神经网络。</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>在ConvNet中，经常会使用一种叫做池化层(pooling layer)的结构来减小表示层的大小，从而加快模型训练，甚至更好的发现一些特性。</p>
<p>下面介绍两种池化层</p>
<h3 id="最大值-Max-池化"><a href="#最大值-Max-池化" class="headerlink" title="最大值(Max)池化"></a>最大值(Max)池化</h3><p>非常简单，如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/maxPooling.png" class="" title="最大值池化">
<p>虽然很简单，但max池化在很多的实际应用中都有很好的效果。为什么呢？<br>直白的理解一下，加入你把这个4×4的区域看作某个特征的集合，即神经网络某个层中的激活状态，那么一个大的数字意味着算法可能检测到了一个特定的特征，如上图左上的2*2区域就有这样的特征，8被挑选并保留下来，它可能是一个垂直边缘，可能是一个任何难以描述的特征，而右上区域没有这样的特征，它的最大值依然很小。<br>所以，max pooling作所做的其实是 ，如果在过滤器中任何地方检测到了任何特征，就保留最大的数值。<br>反之，如果没有明显的特征被检测到，比如右上方的四分之一区域就没有这个特征，在结果中那些数值的最大值仍然相当小。</p>
<h3 id="平均值池化"><a href="#平均值池化" class="headerlink" title="平均值池化"></a>平均值池化</h3><img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/avgPooling.png" class="" title="平均值池化">
<p>就不介绍了…</p>
<h3 id="池化层特点"><a href="#池化层特点" class="headerlink" title="池化层特点"></a>池化层特点</h3><p>最大值池化的使用通常比均值池化多得多。<br>也许你注意到了，大多数池化层都不需要学习超参数，只要确定了f和s，计算就确定了。<br>而且如果输入数据是3维的，则结果中通道数量依然保持，即每一层都像第一层一样进行过滤。</p>
<h2 id="卷积网络样例"><a href="#卷积网络样例" class="headerlink" title="卷积网络样例"></a>卷积网络样例</h2><p>todo…</p>
<h2 id="卷积为什么有用"><a href="#卷积为什么有用" class="headerlink" title="卷积为什么有用"></a>卷积为什么有用</h2><p>比起一般的全连接神经网络，卷积神经网络主要有2个优势，参数共享和连接稀疏性。</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/whyCcnvWork.png" class="" title="例图"> 
<p>举个例子，如上图，第一层共32×32×3 = 3072个输入，第二层共4704个输出。使用如上所示的卷积，我们只需要(5×5×3+1)×6一共456个参数。而如果我们使用全连接层，我们需要多少参数呢？一共3072×4704个参数，多太多了。</p>
<p>卷积网络的参数少，主要是两个原因。</p>
<h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><p>每一次卷积的时候，我们的卷积核一直被轮流地使用，输入数据的每一个部分都与过滤器进行了计算。<br>所谓共享即卷积核的共享。</p>
<h3 id="连接的稀疏性"><a href="#连接的稀疏性" class="headerlink" title="连接的稀疏性"></a>连接的稀疏性</h3><p>也很好理解，比如看上图，卷积后结果矩阵的每一个值，都只是原输入数据的一个3*3的数据与过滤器发生计算得到的，不像全连接要关联所有输入值。</p>
<p>更多，也有人说卷积可以捕捉平移不变，即是说一张图片的像素右移或左移了一些像素，它们依然应该有相同的特征。卷积可以给他们打上相同的标签。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这就是卷积神经网络的基本概念，通过tensorflow等软件包你也可以很轻松的实现各种自定义的网络类型。但一般的实际应用中，并不推荐自由构建网络结构，因为卷积网络的训练时间普遍较长，所以当你要建立一个卷积网络来解决一个新的问题时，去尝试想当然的网络结构往往得不偿失。一个好的方法就是多从各大论文或者成功模型的分享中获得灵感，在别人推荐的模型和超参数的基础上构建你的网络，才能事半功倍。</p>
<p>之后会介绍一些经典的卷积网络模型。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Toulon Du</p>
  <div class="site-description" itemprop="description">Sharing Knowledge And Learn More</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">5</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Toulon Du</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
