<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"toulondu.github.io","root":"/","scheme":"Muse","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Sharing Knowledge And Learn More">
<meta property="og:type" content="website">
<meta property="og:title" content="Toulon&#39;s BLOG">
<meta property="og:url" content="https://toulondu.github.io/index.html">
<meta property="og:site_name" content="Toulon&#39;s BLOG">
<meta property="og:description" content="Sharing Knowledge And Learn More">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Toulon Du">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://toulondu.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>Toulon's BLOG</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Toulon's BLOG</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Algorithm</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/" class="post-title-link" itemprop="url">边写代码边学习Mask-rcnn</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-05-19 00:42:15 / 修改时间：03:35:04" itemprop="dateCreated datePublished" datetime="2020-05-19T00:42:15+08:00">2020-05-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>作为一个深度学习的学习者，你是不是苦于自己看了非常多的理论却难以下手实践？是否总觉得自己还没到写代码的时候？<br>如果你这么想，那么你看再多的理论，也无法真正踏进深度学习的大门。<br>就我个人而言，很多时候即使读完了论文仍然有点懵，很多地方感觉都一知半解，还有些地方可能直接就是不太明白。此时，如果能有源码看一看，才能真正将这篇文章搞明白。<br>so, “talk is cheap, show me the code” 实乃金玉良言。</p>
<p>这篇文章将使用pytorch modelzoo提供的现成Mask R-CNN预训练模型来进行fine-turing，实现一个目标检测和语义分割应用。并且在这个过程中来重新复习一下Mask R-CNN这个经典网络的一些原理。  </p>
<h2 id="Mask-R-CNN简介"><a href="#Mask-R-CNN简介" class="headerlink" title="Mask R-CNN简介"></a>Mask R-CNN简介</h2><p>Mask R-CNN来自何恺明大神2017年的论文，是一个通用的目标检测和实例分割的模型。它基于作者团队在2015年提出的faster rcnn模型，最主要的改动就是增加了一个分支来用于分割任务。<br>Mask R-CNN是anchor-based的模型，依然采用Faster RCNN的2-stage结构，首先用RPN找出候选region，然后在此基础上计算ROI并完成分类、检测和分割任务。<br>并没有添加各种trick，Mask RCNN就超过了当时所有的sota模型。</p>
<h2 id="定义DataSet并处理"><a href="#定义DataSet并处理" class="headerlink" title="定义DataSet并处理"></a>定义DataSet并处理</h2><p>我们将使用Penn-Fudan数据库中的行人图片数据来对模型进行微调。它包含170个图像和345个行人实例。<a href="https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip" target="_blank" rel="noopener">数据在此</a>。<br>数据文件结构大致如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">PennFudanPed&#x2F;</span><br><span class="line">  PedMasks&#x2F;</span><br><span class="line">    FudanPed00001_mask.png</span><br><span class="line">    FudanPed00002_mask.png</span><br><span class="line">    FudanPed00003_mask.png</span><br><span class="line">    FudanPed00004_mask.png</span><br><span class="line">    ...</span><br><span class="line">  PNGImages&#x2F;</span><br><span class="line">    FudanPed00001.png</span><br><span class="line">    FudanPed00002.png</span><br><span class="line">    FudanPed00003.png</span><br><span class="line">    FudanPed00004.png</span><br></pre></td></tr></table></figure>
<p>PedMasks中数据为PNGImages文件夹下对应图片的实例分割掩膜，如下：</p>
<img src="/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/mask_sample.png" class="" title="原图片">
<img src="/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/mask_sample1.png" class="" title="mask图片">
<p>即掩膜中不同的数值对应不同的实例的分割。</p>
<h3 id="定义Dataset类"><a href="#定义Dataset类" class="headerlink" title="定义Dataset类"></a>定义Dataset类</h3><p>上一篇文章说过，Dataset类是帮助我们处理原始数据并产出模型需要的输入数据的类。<br>而在我们这次的Mask R-CNN模型中，我们希望Dataset通过<strong>getitem</strong>能返回我们图像数据(H,W)以及图像的以下信息：</p>
<ul>
<li>boxes: 这张图片里所有的目标区域,格式为[x0,x1,y0,y1]，x∈[0,W], y∈[0,H]</li>
<li>labels: 每个边框的标签</li>
<li>masks: 每个图像的掩膜</li>
<li>image_id: 图片id</li>
<li>area：每个bbox的面积，用于计算IoU</li>
<li>iscrowd: 每个区域是否是人群<br>代码如下：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">class PennFudanDataset(Dataset):</span><br><span class="line">    def __init__(self, root, transforms):</span><br><span class="line">        self.root &#x3D; root</span><br><span class="line">        self.transforms &#x3D; transforms</span><br><span class="line">        # 下载所有图像文件，为其排序。确保它们对齐,而且这样就把图片名字列出来了，方便了加载图片</span><br><span class="line">        self.imgs &#x3D; list(sorted(os.listdir(os.path.join(root, &quot;PNGImages&quot;))))</span><br><span class="line">        self.masks &#x3D; list(sorted(os.listdir(os.path.join(root, &quot;PedMasks&quot;))))</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        # load images ad masks</span><br><span class="line">        img_path &#x3D; os.path.join(self.root, &quot;PNGImages&quot;, self.imgs[idx])</span><br><span class="line">        mask_path &#x3D; os.path.join(self.root, &quot;PedMasks&quot;, self.masks[idx])</span><br><span class="line">        img &#x3D; Image.open(img_path).convert(&quot;RGB&quot;)</span><br><span class="line">        # 请注意我们还没有将mask转换为RGB,</span><br><span class="line">        # 因为每种颜色对应一个不同的实例。0是背景</span><br><span class="line">        mask &#x3D; Image.open(mask_path)</span><br><span class="line">        # 将PIL图像转换为numpy数组</span><br><span class="line">        mask &#x3D; np.array(mask)</span><br><span class="line">        # 实例被编码为不同的颜色</span><br><span class="line">        obj_ids &#x3D; np.unique(mask)</span><br><span class="line">        # 第一个id是背景(即0)，所以删除它</span><br><span class="line">        obj_ids &#x3D; obj_ids[1:]</span><br><span class="line"></span><br><span class="line">        # 将相同颜色编码的mask分成一组</span><br><span class="line">        # mask为2维，用None扩充obj_ids维度，masks为3维，因为一张图片可能有多个实例分割</span><br><span class="line">        # 二进制格式</span><br><span class="line">        masks &#x3D; mask &#x3D;&#x3D; obj_ids[:, None, None]</span><br><span class="line"></span><br><span class="line">        # 获取每个mask的边界框坐标</span><br><span class="line">        num_objs &#x3D; len(obj_ids)</span><br><span class="line">        boxes &#x3D; []</span><br><span class="line">        for i in range(num_objs):</span><br><span class="line">            # masks[i]为2维，所以np.where返回2个tuple，分别为此颜色编码的元素在各个维度的下标</span><br><span class="line">            # 这里的数据中不同颜色的mask是语义分割的像素点，选出最大最小的x坐标和y坐标就得到了目标区域(x0,y0),(x1,y1)</span><br><span class="line">            pos &#x3D; np.where(masks[i])</span><br><span class="line">            xmin &#x3D; np.min(pos[1])</span><br><span class="line">            xmax &#x3D; np.max(pos[1])</span><br><span class="line">            ymin &#x3D; np.min(pos[0])</span><br><span class="line">            ymax &#x3D; np.max(pos[0])</span><br><span class="line">            boxes.append([xmin, ymin, xmax, ymax])</span><br><span class="line"></span><br><span class="line">        # 将所有转换为torch.Tensor</span><br><span class="line">        boxes &#x3D; torch.as_tensor(boxes, dtype&#x3D;torch.float32)</span><br><span class="line">        # 我们只检测行人这一个类(行人，所以直接全部置为1)</span><br><span class="line">        labels &#x3D; torch.ones((num_objs,), dtype&#x3D;torch.int64)</span><br><span class="line">        masks &#x3D; torch.as_tensor(masks, dtype&#x3D;torch.uint8)</span><br><span class="line"></span><br><span class="line">        image_id &#x3D; torch.tensor([idx])</span><br><span class="line">        area &#x3D; (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])</span><br><span class="line">        # 假设所有实例都不是人群</span><br><span class="line">        iscrowd &#x3D; torch.zeros((num_objs,), dtype&#x3D;torch.int64)</span><br><span class="line"></span><br><span class="line">        target &#x3D; &#123;&#125;</span><br><span class="line">        target[&quot;boxes&quot;] &#x3D; boxes  # 这张图片里所有的目标区域</span><br><span class="line">        target[&quot;labels&quot;] &#x3D; labels   # 每个目标区域的类型</span><br><span class="line">        target[&quot;masks&quot;] &#x3D; masks    # 图像掩膜 mask</span><br><span class="line">        target[&quot;image_id&quot;] &#x3D; image_id  # 图片id</span><br><span class="line">        target[&quot;area&quot;] &#x3D; area          # 每个区域的面积</span><br><span class="line">        target[&quot;iscrowd&quot;] &#x3D; iscrowd    # 每个区域是否是人群(这里假设的都不是)</span><br><span class="line"></span><br><span class="line">        if self.transforms is not None:</span><br><span class="line">            img, target &#x3D; self.transforms(img, target)</span><br><span class="line"></span><br><span class="line">        return img, target</span><br><span class="line">        </span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.imgs)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h2><p>Mask-RCNN结构如下：</p>
<img src="/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/mask_rcnn_model.jpg" class="" title="模型结构">
<p>torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) 为我们提供了一个预训练的Mask-RCNN模型。<br>修改这种预训练模型一般有2种思路，第一就是当我们数据较少时，我们就只将最后一层替换成我们的输入目标，然后进行微调。<br>另一种思路则是可以在原模型基础上进行修改，比如替换backbone，修改RPN的anchor数量，调整ROI维度等。  </p>
<p>我们先来看看第二种方式：<br><strong>修改backbone</strong>: Mask-RCNN 的backbone使用的Resnet101，整体还是比较大的，假如你想使用一些轻量的backbone，比如mobileNet，那么你可以进行替换<br><strong>修改rpn</strong>:Mask-RCNN 的anchor是如何生成的呢，注意看上面的结构图。输入数据在经过backbone之后，得到的feature-map其实是在原输入的基础上进行了32倍下采样。基于这个feature-map的每个元素，我们再进行一个3×3的卷积来增加感受野，然后对每个元素生成9个anchor来生成候选区域。这9个初始anchor包含3种长宽比(1:1,1:2,2:1),每种长宽比包含3种不同的面积。结构如下：</p>
<img src="/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/rpn_structure.jpg" class="" title="rpn结构">
<p>注意图上的各种数字，256表示的是骨干网络输出的通道数，k表示生成的anchor的数量。因为每个anchor有positive和negative，所以有2k个打分。而每个anchor会经过后续的回归找到针对正确区域的4个偏移量(x,y,w,h)，所以是4k个coordinates。<br>对于这些参数，我们也可以修改。<br><strong>修改RoI pooling</strong>：RoI pooling层复杂收集proposal，然后选取特征并送入后续的分类和检测FC网络。 要知道为了保留图片中事物的特性，我们很少对图片采用resize或者裁剪操作，而Mask-RCNN接受不同大小的图片输入，那么经过骨干和RPN网络后，各图片到此时的数据维度是不一样的。这种情况下我们没有办法通过FC等网络进行特征组合。RoI pooling层就是来解决这个问题的。它将收集到的proposal分为固定个数的区域(比如7*7)，然后对每这些区域使用max_pool处理，这样就得到了固定维度的输出。<br>其次，Faster RCNN在处理RoI pooling的过程中有2次取整操作：</p>
<ul>
<li>region proposal的xywh通常是小数，但是为了方便操作会把它整数化。</li>
<li>将整数化后的边界区域平均分割成 k x k 个单元，对每一个单元的边界进行整数化。<br>这将会导致RoI pooling后的输出与原图像对应的区域产生一些偏离，导致不能完全对应。第一个问题很好解决，不再取整即可。而解决第二个问题，则是使用双线性插值的方式来更加精确的找到每个边界的特征。我们在下面代码中看到的sampling_ratio=2就是这个方法的体现。</li>
</ul>
<p>代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">from torchvision.models.detection import FasterRCNN</span><br><span class="line">from torchvision.models.detection.rpn import AnchorGenerator</span><br><span class="line"></span><br><span class="line"># 加载预先训练的模型进行分类和返回</span><br><span class="line"># 只有功能 </span><br><span class="line"># 主干采用mobileNet V2</span><br><span class="line">backbone &#x3D; torchvision.models.mobilenet_v2(pretrained&#x3D;True).features</span><br><span class="line"># FasterRCNN需要知道骨干网中的输出通道数量。对于mobilenet_v2，它是1280，所以我们需要在这里添加它</span><br><span class="line">backbone.out_channels &#x3D; 1280</span><br><span class="line"></span><br><span class="line"># 我们让RPN在每个空间位置生成5 x 3个锚点 PS：这里原文是3*3，即3种大小3种宽高比</span><br><span class="line"># 改成5种不同的大小和3种不同的宽高比。 </span><br><span class="line"># 因为每个特征映射可能具有不同的大小和宽高比，size为anchor box大小，aspect_ratios则是宽高比</span><br><span class="line">anchor_generator &#x3D; AnchorGenerator(sizes&#x3D;((32, 64, 128, 256, 512),),</span><br><span class="line">                                   aspect_ratios&#x3D;((0.5, 1.0, 2.0),))</span><br><span class="line"></span><br><span class="line"># 定义一下我们将用于执行感兴趣区域裁剪的特征映射，以及重新缩放后裁剪的大小。 </span><br><span class="line"># 如果您的主干返回Tensor，则featmap_names应为[0]。 </span><br><span class="line"># 更一般地，主干应该返回OrderedDict [Tensor]</span><br><span class="line"># 并且在featmap_names中，您可以选择要使用的功能映射。</span><br><span class="line"># 这里为RoIPooling层，将feature_map对应的原图中部分处理成7*7(output_size&#x3D;7)的大小然后再进行后续的分类和回归操作</span><br><span class="line"># 而sampling_ratio&#x3D;2则是原文中进行插值所选取的采样点，简单的说：采样点为2就是说7*7的每个区域内，都要再分成2*2个grid，然后对每个grid中心点进行采样，将这4个点的值求平均就是这个区域最终的值。</span><br><span class="line">roi_pooler &#x3D; torchvision.ops.MultiScaleRoIAlign(featmap_names&#x3D;[0],</span><br><span class="line">                                                output_size&#x3D;7,</span><br><span class="line">                                                sampling_ratio&#x3D;2)</span><br><span class="line"></span><br><span class="line"># 将这些pieces放在FasterRCNN模型中</span><br><span class="line">model &#x3D; FasterRCNN(backbone,</span><br><span class="line">                   num_classes&#x3D;2,</span><br><span class="line">                   rpn_anchor_generator&#x3D;anchor_generator,</span><br><span class="line">                   box_roi_pool&#x3D;roi_pooler)</span><br></pre></td></tr></table></figure>


<p>虽然第二种方式明显比较酷，但鉴于本示例中样本数据比较少，所以我们使用第一种方式:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def get_model_instance_segmentation(num_classes):</span><br><span class="line">    # 加载在COCO上预训练的预训练的实例分割模型</span><br><span class="line">    model &#x3D; torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained&#x3D;True)</span><br><span class="line"></span><br><span class="line">    # 获取分类器的输入特征数</span><br><span class="line">    in_features &#x3D; model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">    # 用新的头部替换预先训练好的头部</span><br><span class="line">    model.roi_heads.box_predictor &#x3D; FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"></span><br><span class="line">    # 现在获取掩膜分类器的输入特征数</span><br><span class="line">    in_features_mask &#x3D; model.roi_heads.mask_predictor.conv5_mask.in_channels</span><br><span class="line">    hidden_layer &#x3D; 256</span><br><span class="line">    # 并用新的掩膜预测器替换掩膜预测器</span><br><span class="line">    model.roi_heads.mask_predictor &#x3D; MaskRCNNPredictor(in_features_mask,</span><br><span class="line">                                                       hidden_layer,</span><br><span class="line">                                                       num_classes)</span><br></pre></td></tr></table></figure>


<h2 id="实例化模型"><a href="#实例化模型" class="headerlink" title="实例化模型"></a>实例化模型</h2><p>在torchvision的官方库中，references/detection/里有很多辅助函数来简化训练和评估检测模型。<br>这里我们需要用到references/detection/engine.py，references/detection/utils.py和references/detection/transforms.py。<br>去<a href="https://github.com/pytorch/vision" target="_blank" rel="noopener">这里</a> download代码并将这几个文件拷贝到你的目录中即可。</p>
<p>其次，提前安装<a href="https://github.com/cocodataset/cocoapi/tree/master/PythonAPI" target="_blank" rel="noopener">cocoapi</a>,如果你在windows上，可能需要安装visial studio。<br>windows上也可以通过安装pycocotools来解决。whl见：<a href="https://pypi.org/project/pycocotools-windows/#files" target="_blank" rel="noopener">https://pypi.org/project/pycocotools-windows/#files</a></p>
<p>这一步没什么好说的，代码里也有足够的注释：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># 训练阶段按0.5几率水平翻转图像</span><br><span class="line">def get_transform(train):</span><br><span class="line">    transforms &#x3D; []</span><br><span class="line">    transforms.append(T.ToTensor())</span><br><span class="line">    if train:</span><br><span class="line">        transforms.append(T.RandomHorizontalFlip(0.5))</span><br><span class="line">    return T.Compose(transforms)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 在GPU上训练，若无GPU，可选择在CPU上训练</span><br><span class="line">device &#x3D; torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)</span><br><span class="line"></span><br><span class="line"># 我们的数据集只有两个类 - 背景和人</span><br><span class="line">num_classes &#x3D; 2</span><br><span class="line"># 使用我们的数据集和定义的转换</span><br><span class="line">dataset &#x3D; PennFudanDataset(&#39;data&#x2F;PennFudanPed&#39;, get_transform(train&#x3D;True))</span><br><span class="line">dataset_test &#x3D; PennFudanDataset(&#39;data&#x2F;PennFudanPed&#39;, get_transform(train&#x3D;False))</span><br><span class="line"></span><br><span class="line"># 在训练和测试集中拆分数据集</span><br><span class="line">indices &#x3D; torch.randperm(len(dataset)).tolist()</span><br><span class="line">dataset &#x3D; torch.utils.data.Subset(dataset, indices[:-50])</span><br><span class="line">dataset_test &#x3D; torch.utils.data.Subset(dataset_test, indices[-50:])</span><br><span class="line"></span><br><span class="line"># 定义训练和验证数据加载器</span><br><span class="line">data_loader &#x3D; torch.utils.data.DataLoader(</span><br><span class="line">    dataset, batch_size&#x3D;2, shuffle&#x3D;True, num_workers&#x3D;4,</span><br><span class="line">    collate_fn&#x3D;utils.collate_fn)</span><br><span class="line"></span><br><span class="line">data_loader_test &#x3D; torch.utils.data.DataLoader(</span><br><span class="line">    dataset_test, batch_size&#x3D;1, shuffle&#x3D;False, num_workers&#x3D;4,</span><br><span class="line">    collate_fn&#x3D;utils.collate_fn)</span><br><span class="line"></span><br><span class="line"># 使用我们的辅助函数获取模型</span><br><span class="line">model &#x3D; get_model_instance_segmentation(num_classes)</span><br><span class="line"></span><br><span class="line"># 将我们的模型迁移到合适的设备</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>

<h2 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h2><p>我们使用SGD进行优化，训练10个epoch。并且通过比较在测试集上的mAP，保存效果最好的参数到best_state_dict中。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def train():</span><br><span class="line">    # 构造一个优化器</span><br><span class="line">    params &#x3D; [p for p in model.parameters() if p.requires_grad]</span><br><span class="line">    optimizer &#x3D; torch.optim.SGD(params, lr&#x3D;0.005,</span><br><span class="line">                                momentum&#x3D;0.9, weight_decay&#x3D;0.0005)</span><br><span class="line">    # 和学习率调度程序</span><br><span class="line">    lr_scheduler &#x3D; torch.optim.lr_scheduler.StepLR(optimizer,</span><br><span class="line">                                                   step_size&#x3D;3,</span><br><span class="line">                                                   gamma&#x3D;0.1)</span><br><span class="line"></span><br><span class="line">    # 训练10个epochs</span><br><span class="line">    num_epochs &#x3D; 10</span><br><span class="line"></span><br><span class="line">    best_mAp &#x3D; 0</span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        # 训练一个epoch，每10次迭代打印一次</span><br><span class="line">        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq&#x3D;10)</span><br><span class="line">        # 更新学习速率</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        # 在测试集上评价</span><br><span class="line">        eval_res &#x3D; evaluate(model, data_loader_test, device&#x3D;device)</span><br><span class="line"></span><br><span class="line">        # 将结果最好的参数保存下来</span><br><span class="line">        mAp_epoch &#x3D; float(eval_res.coco_eval[&#39;bbox&#39;].stats[0])</span><br><span class="line">        if mAp_epoch &gt; best_mAp:</span><br><span class="line">            torch.save(model.state_dict(),&#39;.&#x2F;best_state_dict&#39;)</span><br><span class="line">            best_mAp &#x3D; mAp_epoch</span><br><span class="line"></span><br><span class="line">    print(&quot;Finish training the model.&quot;)</span><br></pre></td></tr></table></figure>
<p>训练过程中你可以看到各项指标，我忘了截图，最后的COCO-style mAP大概是81左右，mask mAP为在78左右。</p>
<h2 id="使用效果最好的参数进行预测"><a href="#使用效果最好的参数进行预测" class="headerlink" title="使用效果最好的参数进行预测"></a>使用效果最好的参数进行预测</h2><p>完成了训练，接下来肯定就是我们的show time了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(&#39;.&#x2F;best_state_dict&#39;))</span><br><span class="line">    # # 切换为评估模式</span><br><span class="line">    model.eval()</span><br><span class="line"></span><br><span class="line">    # 让我们瞅一瞅效果</span><br><span class="line">    img, _ &#x3D; dataset_test[0]</span><br><span class="line"></span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        prediction &#x3D; model([img.to(device)])</span><br><span class="line"></span><br><span class="line">    img_ori &#x3D; Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())</span><br><span class="line">    draw &#x3D; ImageDraw.Draw(img_ori)</span><br><span class="line"></span><br><span class="line">    masks &#x3D; prediction[0][&#39;masks&#39;]</span><br><span class="line">    masks_all &#x3D; Image.fromarray(np.sum(np.sum(masks.mul(255).byte().cpu().numpy(),axis&#x3D;0),axis&#x3D;0))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    for [x1,y1,x2,y2] in prediction[0][&#39;boxes&#39;]:</span><br><span class="line">        draw.rectangle([(x1,y1),(x2,y2)],outline&#x3D;(255,0,0))</span><br><span class="line"></span><br><span class="line">    imgs &#x3D; [img_ori,masks_all]</span><br><span class="line"></span><br><span class="line">    for i,im in enumerate(imgs):</span><br><span class="line">        ax &#x3D; plt.subplot(1, 2, i + 1)</span><br><span class="line">        plt.tight_layout()</span><br><span class="line">        ax.axis(&#39;off&#39;)</span><br><span class="line">        plt.imshow(im)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>效果如下：</p>
<img src="/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/result01.jpg" class="" title="效果图1">
<img src="/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/result02.jpg" class="" title="效果图2">
<img src="/2020/05/19/%E8%BE%B9%E5%86%99%E4%BB%A3%E7%A0%81%E8%BE%B9%E5%AD%A6%E4%B9%A0mask-rcnn/result03.jpg" class="" title="效果图3">

<h2 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h2><p>本文源码在<a href="https://github.com/toulondu/mask-rcnn-brief" target="_blank" rel="noopener">这里</a></p>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p><a href="https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html" target="_blank" rel="noopener">pytorch官网教程:TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL</a><br><a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN 论文</a><br><a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN 论文</a>、<br><a href="https://zhuanlan.zhihu.com/p/37998710" target="_blank" rel="noopener">令人拍案称奇的Mask RCNN</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/05/15/PyTorch%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/15/PyTorch%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%92%8C%E5%A4%84%E7%90%86/" class="post-title-link" itemprop="url">PyTorch中的数据加载和处理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-15 14:21:23" itemprop="dateCreated datePublished" datetime="2020-05-15T14:21:23+08:00">2020-05-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-19 00:44:34" itemprop="dateModified" datetime="2020-05-19T00:44:34+08:00">2020-05-19</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E7%BC%96%E7%A0%81%E5%B7%A5%E5%85%B7/" itemprop="url" rel="index"><span itemprop="name">编码工具</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>这篇文章来自<a href="http://pytorch123.com/ThirdSection/DataLoding/" target="_blank" rel="noopener">官网中文教程</a>。<br>我稍稍作了总结，并把代码中稍微不好理解的地方做了详细的注释。</p>
<p>PyTorch提供了许多工具来简化和进行数据加载，使代码更具可读性。<br>这一节就主要介绍pytorch中是如何进行数据的加载和处理的。</p>
<p>先安装2个包：</p>
<ul>
<li>scikit-image：用于图像的IO和变换</li>
<li>pandas：用于更容易地进行csv解析</li>
</ul>
<p>在pytorch中进行数据加载，主要的步骤是：</p>
<ol>
<li>将下好的数据集放在本地文件夹中，本示例中的数据集是imagenet数据集标注为face的图片当中在 dlib 面部检测 (dlib’s pose estimation) 表现良好的图片，连接在<a href="https://download.pytorch.org/tutorial/faces.zip" target="_blank" rel="noopener">这里</a>。</li>
<li>先创建一个自己的数据集类(继承自dataset),在<strong>init</strong>中读取数据内容(根据数据的来源)，在<strong>getitem</strong>中根据idx读取文件。</li>
<li>在Dataset类中可以加入一个transform参数，它是一个函数，可以对样本数据进行预处理，包括Rescale,randomCrop等(输入图片大小不符合网络输入要求时)。我们可以通过torchvision.transforms.Compose这个函数式的方法将数据转换的方法合成一个方法作为我们最后的transform。</li>
<li>迭代从我们创建的Dataset中获取数据，这不需要我们自己实现，通过<strong>torch.utils.data.DataLoader</strong>提供的多线程实现我们可以更高效地载入数据。但在Windows上会存在内存泄漏的问题，所以无法使用多线程。</li>
<li>torchvision，这个包提供了常用的datasets和transform。</li>
</ol>
<p>代码如下,内部有详细注释</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br></pre></td><td class="code"><pre><span class="line">from __future__ import print_function, division</span><br><span class="line">import os</span><br><span class="line">import torch</span><br><span class="line">import pandas as pd              #用于更容易地进行csv解析</span><br><span class="line">from skimage import io, transform    #用于图像的IO和变换</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from torch.utils.data import Dataset, DataLoader</span><br><span class="line">from torchvision import transforms, utils</span><br><span class="line"></span><br><span class="line"># 忽略警告</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(&quot;ignore&quot;)</span><br><span class="line"></span><br><span class="line">plt.ion()   # interactive mode</span><br><span class="line"></span><br><span class="line"># 下载数据集放在&#39;data&#x2F;face&#39;中，</span><br><span class="line"># 这个数据集实际上是imagenet数据集标注为face的图片当中在 dlib 面部检测 (dlib’s pose estimation) 表现良好的图片。</span><br><span class="line"># 我们要处理的是一个面部姿态的数据集。</span><br><span class="line"># 数据集是按如下规则打包成的csv文件:</span><br><span class="line"># image_name,part_0_x,part_0_y,part_1_x,part_1_y,part_2_x, ... ,part_67_x,part_67_y</span><br><span class="line"></span><br><span class="line"># 将csv中的标注点数据读入（N，2）数组中，其中N是特征点的数量。读取数据代码如下：</span><br><span class="line">landmarks_frame &#x3D; pd.read_csv(&#39;data&#x2F;faces&#x2F;face_landmarks.csv&#39;)</span><br><span class="line"></span><br><span class="line">n &#x3D; 32</span><br><span class="line">img_name &#x3D; landmarks_frame.iloc[n, 0]</span><br><span class="line"># values将Series作为ndarry或者ndarry-like类型的数据返回，取决于dtype类型. 这句代码取出了第n张图片的所以标记点信息</span><br><span class="line">landmarks &#x3D; landmarks_frame.iloc[n, 1:].values</span><br><span class="line"># 一对一对的坐标，2个一组</span><br><span class="line">landmarks &#x3D; landmarks.astype(&#39;float&#39;).reshape(-1, 2)</span><br><span class="line"></span><br><span class="line">print(&#39;Image name: &#123;&#125;&#39;.format(img_name))</span><br><span class="line">print(&#39;Landmarks shape: &#123;&#125;&#39;.format(landmarks.shape))</span><br><span class="line">print(&#39;First 4 Landmarks: &#123;&#125;&#39;.format(landmarks[:4]))</span><br><span class="line"></span><br><span class="line"># 展示一张图片和它对应的标注点作为例子。用到的函数都很容易看出作用</span><br><span class="line">def show_landmarks(image, landmarks):</span><br><span class="line">    &quot;&quot;&quot;显示带有地标的图片&quot;&quot;&quot;</span><br><span class="line">    plt.imshow(image)</span><br><span class="line">    plt.scatter(landmarks[:, 0], landmarks[:, 1], s&#x3D;10, marker&#x3D;&#39;.&#39;, c&#x3D;&#39;r&#39;)</span><br><span class="line">    plt.pause(0.001)  # pause a bit so that plots are updated</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">show_landmarks(io.imread(os.path.join(&#39;data&#x2F;faces&#x2F;&#39;, img_name)),</span><br><span class="line">               landmarks)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据集类</span><br><span class="line"># torch.utils.data.Dataset是表示数据集的抽象类，因此自定义数据集应继承Dataset并覆盖以下方法 </span><br><span class="line"># * __len__ 实现 len(dataset) 返还数据集的尺寸。</span><br><span class="line"># * __getitem__用来获取一些索引数据，例如 dataset[i] 中的(i)。</span><br><span class="line"></span><br><span class="line"># 为面部数据集创建一个数据集类。</span><br><span class="line"># 我们将在 __init__中读取csv的文件内容，在 __getitem__中读取图片。</span><br><span class="line"># 这么做是为了节省内存 空间。只有在需要用到图片的时候才读取它而不是一开始就把图片全部存进内存里。</span><br><span class="line"></span><br><span class="line"># 我们的数据样本将按这样一个字典&#123;&#39;image&#39;: image, &#39;landmarks&#39;: landmarks&#125;组织。 </span><br><span class="line"># 我们的数据集类将添加一个可选参数transform 以方便对样本进行预处理。</span><br><span class="line">#下一节我们会看到什么时候需要用到transform参数。 __init__方法如下图所示：</span><br><span class="line"># 这部分代码也没什么值得说的</span><br><span class="line">class FaceLandmarksDataset(Dataset):</span><br><span class="line">    &quot;&quot;&quot;面部标记数据集.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, csv_file, root_dir, transform&#x3D;None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        csv_file（string）：带注释的csv文件的路径。</span><br><span class="line">        root_dir（string）：包含所有图像的目录。</span><br><span class="line">        transform（callable， optional）：一个样本上的可用的可选变换</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.landmarks_frame &#x3D; pd.read_csv(csv_file)</span><br><span class="line">        self.root_dir &#x3D; root_dir</span><br><span class="line">        self.transform &#x3D; transform</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.landmarks_frame)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        img_name &#x3D; os.path.join(self.root_dir,</span><br><span class="line">                                self.landmarks_frame.iloc[idx, 0])</span><br><span class="line">        image &#x3D; io.imread(img_name)</span><br><span class="line">        landmarks &#x3D; self.landmarks_frame.iloc[idx, 1:]</span><br><span class="line">        landmarks &#x3D; np.array([landmarks])</span><br><span class="line">        landmarks &#x3D; landmarks.astype(&#39;float&#39;).reshape(-1, 2)</span><br><span class="line">        sample &#x3D; &#123;&#39;image&#39;: image, &#39;landmarks&#39;: landmarks&#125;</span><br><span class="line"></span><br><span class="line">        if self.transform:</span><br><span class="line">            sample &#x3D; self.transform(sample)</span><br><span class="line"></span><br><span class="line">        return sample</span><br><span class="line">        </span><br><span class="line"># 数据可视化</span><br><span class="line"># 实例化这个类并遍历数据样本。我们将会打印出前四个例子的尺寸并展示标注的特征点。 代码如下图所示：</span><br><span class="line">face_dataset &#x3D; FaceLandmarksDataset(csv_file&#x3D;&#39;data&#x2F;faces&#x2F;face_landmarks.csv&#39;,</span><br><span class="line">                                    root_dir&#x3D;&#39;data&#x2F;faces&#x2F;&#39;)</span><br><span class="line"></span><br><span class="line">fig &#x3D; plt.figure()</span><br><span class="line"></span><br><span class="line">for i in range(len(face_dataset)):</span><br><span class="line">    sample &#x3D; face_dataset[i]</span><br><span class="line"></span><br><span class="line">    print(i, sample[&#39;image&#39;].shape, sample[&#39;landmarks&#39;].shape)</span><br><span class="line">    </span><br><span class="line">    # 创建子图，1行4列，最后一个参数为当前图编号 但也不知道为什么没成功，还是画成了4行1列</span><br><span class="line">    ax &#x3D; plt.subplot(1, 4, i + 1)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    ax.set_title(&#39;Sample #&#123;&#125;&#39;.format(i))</span><br><span class="line">    # 不画坐标轴</span><br><span class="line">    ax.axis(&#39;off&#39;)</span><br><span class="line">    show_landmarks(**sample)</span><br><span class="line"></span><br><span class="line">    if i &#x3D;&#x3D; 3:</span><br><span class="line">        plt.show()</span><br><span class="line">        break</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 数据变换</span><br><span class="line"># 通过上面的例子我们会发现图片并不是同样的尺寸。绝大多数神经网络都假定图片的尺寸相同。因此我们需要做一些预处理。</span><br><span class="line"># 让我们创建三个转换: * Rescale：缩放图片 * RandomCrop：对图片进行随机裁剪。这是一种数据增强操作 * ToTensor：把numpy格式图片转为torch格式图片 (我们需要交换坐标轴).</span><br><span class="line"># 我们会把它们写成可调用的类的形式而不是简单的函数，这样就不需要每次调用时传递一遍参数。我们只需要实现__call__方法，必 要的时候实现 __init__方法。我们可以这样调用这些转换:</span><br><span class="line"># tsfm &#x3D; Transform(params)</span><br><span class="line"># transformed_sample &#x3D; tsfm(sample)</span><br><span class="line"></span><br><span class="line">class Rescale(object):</span><br><span class="line">    &quot;&quot;&quot;将样本中的图像重新缩放到给定大小。.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">        output_size（tuple或int）：所需的输出大小。 如果是元组，则输出为</span><br><span class="line">         与output_size匹配。 如果是int，则匹配较小的图像边缘到output_size保持纵横比相同。</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, output_size):</span><br><span class="line">        assert isinstance(output_size, (int, tuple))</span><br><span class="line">        self.output_size &#x3D; output_size</span><br><span class="line"></span><br><span class="line">    def __call__(self, sample):</span><br><span class="line">        image, landmarks &#x3D; sample[&#39;image&#39;], sample[&#39;landmarks&#39;]</span><br><span class="line"></span><br><span class="line">        h, w &#x3D; image.shape[:2]</span><br><span class="line">        if isinstance(self.output_size, int):</span><br><span class="line">            # output_size为int，将长或宽中小的那一个rescale成output_size，另一个保持纵横比缩放</span><br><span class="line">            if h &gt; w:</span><br><span class="line">                new_h, new_w &#x3D; self.output_size * h &#x2F; w, self.output_size</span><br><span class="line">            else:</span><br><span class="line">                new_h, new_w &#x3D; self.output_size, self.output_size * w &#x2F; h</span><br><span class="line">        else:</span><br><span class="line">            new_h, new_w &#x3D; self.output_size</span><br><span class="line"></span><br><span class="line">        new_h, new_w &#x3D; int(new_h), int(new_w)</span><br><span class="line">        </span><br><span class="line">        # 看来skimage的这个库实现了对图片的各种转换操作</span><br><span class="line">        img &#x3D; transform.resize(image, (new_h, new_w))</span><br><span class="line"></span><br><span class="line">        # 别忘了对landmarks也要作同样比例的缩放！</span><br><span class="line">        landmarks &#x3D; landmarks * [new_w &#x2F; w, new_h &#x2F; h]</span><br><span class="line"></span><br><span class="line">        return &#123;&#39;image&#39;: img, &#39;landmarks&#39;: landmarks&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class RandomCrop(object):</span><br><span class="line">    &quot;&quot;&quot;随机裁剪样本中的图像.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">       output_size（tuple或int）：所需的输出大小。 如果是int，就按照int的值进行方形裁剪。         </span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, output_size):</span><br><span class="line">        assert isinstance(output_size, (int, tuple))</span><br><span class="line">        if isinstance(output_size, int):</span><br><span class="line">            self.output_size &#x3D; (output_size, output_size)</span><br><span class="line">        else:</span><br><span class="line">            assert len(output_size) &#x3D;&#x3D; 2</span><br><span class="line">            self.output_size &#x3D; output_size</span><br><span class="line"></span><br><span class="line">    def __call__(self, sample):</span><br><span class="line">        image, landmarks &#x3D; sample[&#39;image&#39;], sample[&#39;landmarks&#39;]</span><br><span class="line"></span><br><span class="line">        h, w &#x3D; image.shape[:2]</span><br><span class="line">        new_h, new_w &#x3D; self.output_size</span><br><span class="line">        </span><br><span class="line">        # 随机从图片中裁剪一块目标大小的图像出来,landmark也作相应处理 PS:h更小肿么办</span><br><span class="line">        top &#x3D; np.random.randint(0, h - new_h)</span><br><span class="line">        left &#x3D; np.random.randint(0, w - new_w)</span><br><span class="line"></span><br><span class="line">        image &#x3D; image[top: top + new_h,</span><br><span class="line">                      left: left + new_w]</span><br><span class="line"></span><br><span class="line">        landmarks &#x3D; landmarks - [left, top]</span><br><span class="line"></span><br><span class="line">        return &#123;&#39;image&#39;: image, &#39;landmarks&#39;: landmarks&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ToTensor(object):</span><br><span class="line">    &quot;&quot;&quot;将样本中的ndarrays转换为Tensors.&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __call__(self, sample):</span><br><span class="line">        image, landmarks &#x3D; sample[&#39;image&#39;], sample[&#39;landmarks&#39;]</span><br><span class="line"></span><br><span class="line">        # 交换颜色轴因为</span><br><span class="line">        # numpy包的图片是: H * W * C</span><br><span class="line">        # torch包的图片是: C * H * W</span><br><span class="line">        image &#x3D; image.transpose((2, 0, 1))</span><br><span class="line">        return &#123;&#39;image&#39;: torch.from_numpy(image),</span><br><span class="line">                &#39;landmarks&#39;: torch.from_numpy(landmarks)&#125;</span><br><span class="line">                </span><br><span class="line"># 8.组合转换</span><br><span class="line"># 即把这些转换应用起来 </span><br><span class="line"># 我们想要把图像的短边调整为256，然后随机裁剪(randomcrop)为224大小的正方形。</span><br><span class="line"># 也就是说，我们打算组合一个Rescale和 RandomCrop的变换。 </span><br><span class="line"># 我们可以调用一个简单的类 torchvision.transforms.Compose来实现这一操作。具体实现如下图：</span><br><span class="line"></span><br><span class="line">scale &#x3D; Rescale(256)</span><br><span class="line">crop &#x3D; RandomCrop(128)</span><br><span class="line">composed &#x3D; transforms.Compose([Rescale(256),</span><br><span class="line">                               RandomCrop(224)])</span><br><span class="line"></span><br><span class="line"># 在样本上应用上述的每个变换。</span><br><span class="line">fig &#x3D; plt.figure()</span><br><span class="line">sample &#x3D; face_dataset[65]</span><br><span class="line"></span><br><span class="line">for i, tsfrm in enumerate([scale, crop, composed]):</span><br><span class="line">    transformed_sample &#x3D; tsfrm(sample)</span><br><span class="line">    </span><br><span class="line">    ax &#x3D; plt.subplot(1, 3, i + 1)</span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    ax.set_title(type(tsfrm).__name__)</span><br><span class="line">    show_landmarks(**transformed_sample)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># 9.迭代数据集</span><br><span class="line"># 让我们把这些整合起来以创建一个带组合转换的数据集。</span><br><span class="line"># 总结一下，每次这个数据集被采样时: </span><br><span class="line"># * 及时地从文件中读取图片 * 对读取的图片应用转换 * 由于其中一步操作是随机的 (randomcrop) , 数据被增强了</span><br><span class="line"></span><br><span class="line"># 我们可以像之前那样使用for i in range循环来对所有创建的数据集执行同样的操作。</span><br><span class="line"></span><br><span class="line"># 还记得FaceLandmarksDataset 这个类吗，忘了可以翻回去看一下</span><br><span class="line">transformed_dataset &#x3D; FaceLandmarksDataset(csv_file&#x3D;&#39;data&#x2F;faces&#x2F;face_landmarks.csv&#39;,</span><br><span class="line">                                           root_dir&#x3D;&#39;data&#x2F;faces&#x2F;&#39;,</span><br><span class="line">                                           transform&#x3D;transforms.Compose([</span><br><span class="line">                                               Rescale(256),</span><br><span class="line">                                               RandomCrop(224),</span><br><span class="line">                                               ToTensor()</span><br><span class="line">                                           ]))</span><br><span class="line"></span><br><span class="line">for i in range(len(transformed_dataset)):</span><br><span class="line">    sample &#x3D; transformed_dataset[i]</span><br><span class="line"></span><br><span class="line">    print(i, sample[&#39;image&#39;].size(), sample[&#39;landmarks&#39;].size())</span><br><span class="line"></span><br><span class="line">    if i &#x3D;&#x3D; 3:</span><br><span class="line">        break</span><br><span class="line">        </span><br><span class="line"># 但是，对所有数据集简单的使用for循环牺牲了许多功能，尤其是: * 批量处理数据 * 打乱数据 * 使用多线程multiprocessingworker 并行加载数据。</span><br><span class="line"># torch.utils.data.DataLoader是一个提供上述所有这些功能的迭代器。</span><br><span class="line"># 下面使用的参数必须是清楚的。一个值得关注的参数是collate_fn, 可以通过它来决定如何对数据进行批处理。但是绝大多数情况下默认值就能运行良好。</span><br><span class="line"># PS: 这里windows下跑是有问题的，因为windows在用这个多线程方法时有内存泄漏，详情：https:&#x2F;&#x2F;github.com&#x2F;pytorch&#x2F;pytorch&#x2F;pull&#x2F;5585</span><br><span class="line"># 概括点来说就是，因为在Windows上使用FileMapping（mmap）的差异引起的。</span><br><span class="line"># Windows上，所有相关线程都释放了FileMapping对象的引用，它才才能够被释放。没有提供其他方法可以将其直接删除。（例如shm_unlink） </span><br><span class="line"># 启用多线程后，子线程将创建一个FileMapping，然后主进程将其打开。之后子线程将尝试释放它，但是它的引用计数非零，因此无法在那时释放它。</span><br><span class="line"># 而且当前代码无法提供在可能的情况下再次关闭的机会，于是导致了内存泄漏。</span><br><span class="line"></span><br><span class="line"># 解决办法是权宜之计，即使用num_workers参数，令num_workers&#x3D;0,只使用一个主线程加载数据集。避免在windows中使用多线程。</span><br><span class="line"></span><br><span class="line">dataloader &#x3D; DataLoader(transformed_dataset, batch_size&#x3D;4,</span><br><span class="line">                        shuffle&#x3D;True, num_workers&#x3D;4)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 辅助功能：显示批次</span><br><span class="line">def show_landmarks_batch(sample_batched):</span><br><span class="line">    &quot;&quot;&quot;Show image with landmarks for a batch of samples.&quot;&quot;&quot;</span><br><span class="line">    images_batch, landmarks_batch &#x3D; \</span><br><span class="line">            sample_batched[&#39;image&#39;], sample_batched[&#39;landmarks&#39;]</span><br><span class="line">    batch_size &#x3D; len(images_batch)</span><br><span class="line">    im_size &#x3D; images_batch.size(2)</span><br><span class="line">    grid_border_size &#x3D; 2</span><br><span class="line"></span><br><span class="line">    grid &#x3D; utils.make_grid(images_batch)</span><br><span class="line">    plt.imshow(grid.numpy().transpose((1, 2, 0)))</span><br><span class="line">    </span><br><span class="line">    for i in range(batch_size):</span><br><span class="line">        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size + (i + 1) * grid_border_size,</span><br><span class="line">                    landmarks_batch[i, :, 1].numpy() + grid_border_size,</span><br><span class="line">                    s&#x3D;10, marker&#x3D;&#39;.&#39;, c&#x3D;&#39;r&#39;)</span><br><span class="line"></span><br><span class="line">        plt.title(&#39;Batch from dataloader&#39;)</span><br><span class="line">        </span><br><span class="line">for i_batch, sample_batched in enumerate(dataloader):</span><br><span class="line">    print(i_batch, sample_batched[&#39;image&#39;].size(),</span><br><span class="line">          sample_batched[&#39;landmarks&#39;].size())</span><br><span class="line"></span><br><span class="line">    # 观察第4批次并停止。</span><br><span class="line">    if i_batch &#x3D;&#x3D; 3:</span><br><span class="line">        plt.figure()</span><br><span class="line">        show_landmarks_batch(sample_batched)</span><br><span class="line">        plt.axis(&#39;off&#39;)</span><br><span class="line">        plt.ioff()</span><br><span class="line">        plt.show()</span><br><span class="line">        break</span><br></pre></td></tr></table></figure>

<p><strong>torchvision</strong><br>torchvision包提供了 常用的数据集类(datasets)和转换(transforms)。<br>所以只要我们的数据符合这些数据集的要求，我们就不需要自己构造这些类比如。torchvision中还有一个更常用的数据集类ImageFolder。 它假定了数据集是以如下方式构造的:<br>root/ants/xxx.png<br>root/ants/xxy.jpeg<br>root/ants/xxz.png<br>.<br>.<br>.<br>root/bees/123.jpg<br>root/bees/nsdf3.png<br>root/bees/asd932_.png</p>
<p>其中’ants’,bees’等是分类标签。在PIL.Image中你也可以使用类似的转换(transforms)例如RandomHorizontalFlip,Scale。利 用这些你可以按如下的方式创建一个数据加载器(dataloader) :</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">from torchvision import transforms, datasets</span><br><span class="line"></span><br><span class="line">data_transform &#x3D; transforms.Compose([</span><br><span class="line">        transforms.RandomSizedCrop(224),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean&#x3D;[0.485, 0.456, 0.406],</span><br><span class="line">                             std&#x3D;[0.229, 0.224, 0.225])</span><br><span class="line">    ])</span><br><span class="line">hymenoptera_dataset &#x3D; datasets.ImageFolder(root&#x3D;&#39;hymenoptera_data&#x2F;train&#39;,</span><br><span class="line">                                           transform&#x3D;data_transform)</span><br><span class="line">dataset_loader &#x3D; torch.utils.data.DataLoader(hymenoptera_dataset,</span><br><span class="line">                                             batch_size&#x3D;4, shuffle&#x3D;True,</span><br><span class="line">                                             num_workers&#x3D;4)</span><br></pre></td></tr></table></figure>




















      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/" class="post-title-link" itemprop="url">推荐系统技术概览</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-05-01 18:32:57" itemprop="dateCreated datePublished" datetime="2020-05-01T18:32:57+08:00">2020-05-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-12 17:38:51" itemprop="dateModified" datetime="2020-05-12T17:38:51+08:00">2020-05-12</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">基础算法</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>最近在一边写一个web qa的问答模型一边刷leetcode，前者踩坑太多耗费了超出我预期的时间(还是攻城能力欠缺啊- -)，于是文章也有一段时间没更新了。<br>这期间抽空回顾和学习了目前工业界推荐系统常用的架构和算法，也亲自用代码实现了其中的部分算法，但总体还是停留在纸上谈兵的阶段。<br>虽然这篇文章我脸很厚的用了<strong><em>“概览”</em></strong>这个词，但我们都知道真实的工程落地远比纯粹的算法来得复杂，除了对应实际问题的难度，还会遭遇很多预想不到的困难，当然，这是题外话了~</p>
<p>这篇文章基本是对我近期看的这些推荐系统算法文章和视频的一个个人的文字总结，希望自己之后有应用的机会吧~</p>
<p>相关链接会放在文末。</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>工业界推荐系统的架构基本在宏观上比较趋同，大都可以粗分为<strong>召回(recall)</strong>和<strong>排序(rank)</strong>两个阶段。如下：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/recommandation-system-structure.png" class="" title="推荐系统架构">

<p>一般来说，平台往往动辄有上百万的item，不管是新闻应用，视频应用还是电商应用，要从海量的item中直接一步到位准确地找到推荐给某个用户的item是比较难的。 这两个阶段可以大概地看成是粗筛和精选，对某个用户来说，先通过一个比较快速而简单的方法从上百万的item中筛选出几百或者几千个高质量的item，再通过一个精确的方法仔细地将其中最好的一些item优先级确定下来，然后推荐给用户。前者就叫做召回阶段，后者则叫做排序阶段。</p>
<p>当然，落地的时候往往还会把排序阶段再分成 粗排-主排-重排，于是变成了四个阶段，这个后面再说~</p>
<p>下面就来介绍一下各个阶段常用的算法。</p>
<h2 id="召回算法"><a href="#召回算法" class="headerlink" title="召回算法"></a>召回算法</h2><p>召回算法有很多，主要分为3类：</p>
<ul>
<li>基于用户行为，简单地说就是“你看了什么，我就给你推荐什么“</li>
<li>基于用户档案，即给用户建立档案，根据用户档案中的标签推荐相应的item</li>
<li>基于隐语义，即基于机器学习方法，使用类似于embedding的方式<br>三者各有特点和优劣，基于用户行为直观上显得过于简单，基于用户档案也会存在用户改口味难以泛化的情况，而机器学习，本身就偏向黑盒，对于算法中的中间值难以解释，从而优化和修改显得困难。</li>
</ul>
<p>所以在使用这些算法的时候，一般是使用多路召回，即使用很多不同的召回算法分别进行召回，再把各自召回的结果组合起来作为最后的召回结果。</p>
<p>这里主要介绍 CF, personal rank, item2vec等算法。</p>
<h3 id="CF-协同过滤"><a href="#CF-协同过滤" class="headerlink" title="CF(协同过滤)"></a>CF(协同过滤)</h3><p>协同过滤是一种很老的算法，但沿用至今，它能够实现对特征进行学习的算法，即能够自行学习所需要使用的特征。</p>
<p>算法的核心是通过学习得到2个维度较低的矩阵，一个代表用户的embedding矩阵，一个代表item的embedding矩阵，计算某个用户对于item的推荐值的时候，只需要将二者进行点积计算。再直白一点，比如电影拥有两个特征，动作属性和爱情属性，某用户A的embedding向量为θ=[5，1]，即A对电影中动作元素的偏爱为5，相对地对爱情元素的偏爱只有1。而某一部电影的X=embedding为[4，1]，那么我们将这2个向量直接进行点积，matmul(θ,transpose(X))，得到的值就是这部电影对于这个用户的推荐值，将不同电影的推荐值进行排序，其中推荐值最高的不就是应该推荐给A的电影了吗。</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/CF01.png" class="" title="CF图示">

<p>怎么得到这两个embedding矩阵呢？基于上面电影推荐的例子，假设我们知道每部电影的特征矩阵，即X已知 求θ。那这个问题就变成了一个很简单的线性回归问题。 已有的打分数据作为样本，我们的目标函数只需要将 θ·XT减去真实评分 作为误差，很简单就可以定义出目标函数：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/CF-formula-1.png" class="" title="CF单体公式">

<p>其中</p>
<ul>
<li>θ^(j)代表用户j的embedding向量。</li>
<li>x^(i)则代表电影i的embedding向量。</li>
<li>r(i,j)为1表示用户j已经为电影i打了分，0则表示没有。</li>
<li>y^(i,j)为用户j给电影i的评分<br>将这个公式推广到所有用户：<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/CF-formula-1.png" class="" title="CF公式">

</li>
</ul>
<p>有了目标函数我们在将其对θ求导</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/CF-derivative-theta.png" class="" title="Lossd对θ导数">

<p>然后直接用梯度下降之类的优化算法进行求解即可。</p>
<p>同理，假设θ已知，我们也可以用同样的方式求得X。</p>
<p>你可能会想，这不就成了鸡生蛋还是蛋生鸡的问题了吗？问题是两者我们都不知道啊？</p>
<p>是的，你很清醒，没有被带偏，实际情况中，用户的偏好和电影的属性都很难收集，更别提特征这个东西本就虚无缥缈，爱情属性动作属性你还能够理解，真出来几百个特征，你能分清什么是什么吗?</p>
<p>这个时候协同过滤就钻出来了，在面对这种两个未知数可以互相更新的情况下，最好的方式就是初始化其中一个未知数，然后求得另一个未知数，再反过来求第一个未知数，以此类推，在这个过程中，2个未知数就会产生协同作用，终会完成收敛，达到生命的大和谐。想想pagerank,想想EM，是不是很熟悉~</p>
<p>除了让二者彼此更新，还可以将二者同时计算：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/CF-form.png" class="" title="协同过滤">

<p>第一个式子是对每个用户，通过他们评价过的电影的类别来推断用户的喜好。第二个式子反过来，对每个电影，找出评价过它的用户的喜好来推断电影的类别。</p>
<p>其实二者前半部分就是第三个式子中的第一部分，只是第三个式子取得是一个(i,j)的对，表示所有用户评分过的电影。然后把二者的正则化都加上。实际上关于第三个式子，假如你假设x为常数，它就等于第一个式子，你假设θ为常数，它就等于第二个式子。</p>
<p>这就是协同过滤，实际实现过程中是可以用向量化实现从而不需要一个一个地进行计算的。这样的实现也叫做<strong>低秩矩阵分解(low rank matrix factorization)</strong>，还有一些地方叫做<strong>LFM(latent factor model)</strong>。不过矩阵分解方法并不是只此一家，SVD也是其中的佼佼者。这些方法最终都是得到user-item的隐式矩阵分解，获取二者的隐相量。</p>
<h3 id="personal-rank"><a href="#personal-rank" class="headerlink" title="personal rank"></a>personal rank</h3><p>推荐系统中最基础的两个部分就是user和item，整个系统也是user和item的交互，很容易想到，这就是一个图结构。那么，聪明的先行者们就想，能不能使用图算法来进行个性化推荐呢？ 当然可以~</p>
<p>user和item构成的图是一个<strong>二分图</strong>。</p>
<p>摘抄一下百度百科二分图的解释：<br>二分图又称作二部图，是图论中的一种特殊模型。 设G=(V,E)是一个无向图，如果顶点V可分割为两个互不相交的子集(A,B)，并且图中的每条边（i，j）所关联的两个顶点i和j分别属于这两个不同的顶点集(i in A,j in B)，则称图G为一个二分图。</p>
<p>而对于推荐系统而言，用户和item刚好是一个二分图，顶点分为user和item两类，且所有的变都是连接一个user和一个item的。</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/personal_rank_example.png" class="" title="示例">
<p>如上user和item构成了一个二分图，大写ABCD表示user，小写表示item。<br>personal rank比较各个item对于某个user的推荐值基于以下规则，其重要性一次递减：</p>
<ol>
<li>两个顶点间有多少条路径可以连通，如上图，从A-c，有A-a-B-c, A-d-D-c两条，而从A-e只有A-b-C-e一条，则c更值得推荐。</li>
<li>如果第一条相同，则比较连同路径的总长度，长度短者更值得推荐</li>
<li>如果1，2相同，则比较连同路径经过的顶点的出度和。</li>
</ol>
<p>基于此规则，我们来介绍personal rank方法的详细操作：<br>将二分图视作无向图，对于用户A进行推荐时，我们就从A节点出发开始在图上进行随机游走，以概率α从A所有的出度中等概率选择一条前进，到达对应顶点后(比如到了b)，再次以α的概率继续从a的出度中 等概率地选择一条继续前进，或者(1-α)的概率回到起点A，经历很多步之后，统计A到达各个item节点的次数求得概率。可以证明，只要步数足够大，此概率可以收敛。</p>
<p>把固定item对固定user的推荐得分记作PR值</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/personal_rank_formula.png" class="" title="personal_rank公式">
<p>其中out(v~)表示节点的出度。</p>
<p>可以看出，和pagerank算法非常相似，某个点的PR值等于 可以连通到该点的其它点的PR值除以自身出度 的和，即如a，有A,B可以连接到它，而A出度为3，B出度为2，故PR(a) = PR(A)/3 + PR(B)/2，再乘以一个概率α。A自身的PR值还要加上一个(1-alpha)。</p>
<p>直接迭代计算，复杂度太高了，像pagerank一样，我们也可以祭出矩阵来做：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/personal_rank_form_vec.png" class="" title="向量化实现">
<ul>
<li>r是一个n维向量，n是图中所有的节点数量，包括user节点和item节点，值则是每个节点的PR值。</li>
<li>r0也是一个n维向量，只有对应出发节点的元素为1，其余节点为0。</li>
<li>M是一个n阶转移矩阵，即图中节点与节点之间转移的概率，很好理解。<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/trans_M.png" class="" title="转移矩阵M">
这样不断将结果代入公式，r最终可以收敛。</li>
</ul>
<p>再对公式进行移项处理，得到：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/personal_rank_final_form.png" class="" title="移项处理后">

<p>向量化实现很简单，你把r0换成矩阵试试。</p>
<p>PS：</p>
<ul>
<li>在计算时很多喜欢去掉1-α这个值，是因为推荐时我们更多的是要的推荐排序结果，而不是具体的推荐值，故去掉一个常数乘子没有影响。</li>
<li>可以看到其中M是稀疏矩阵，故E-αMT也是稀疏矩阵。针对它的存储和计算都有很多成熟的方法。</li>
</ul>
<h3 id="item2vec"><a href="#item2vec" class="headerlink" title="item2vec"></a>item2vec</h3><p>item2vec是一个神经网络算法，它基本是属于word2vec的衍生品。word2vec算法在诞生后，其简洁的方法和出色的效果掀起了一股万物皆可embedding的热潮，item2vec就是将word2vec应用到推荐系统上的算法。</p>
<p>所以要说item2vec，就逃不了word2vec，word2vec是一个用于NLP的算法，由大神Tomas Mikolov在2013年的论文《Efficient estimation of word representations in vector space》中提出。word2vec是一篇跨时代的论文，在word2vec诞生前，语言中的词都是使用one-hot、tf-inf等统计学方法或者NNLM这种计算量非常大的n-gram方式进行表示的，这样就会使得数据维度特别高(语料库大小)和精度低，而且词与词的关系也被忽略了。word2vec是一种将其进行降维，并且可以表示出词与词关系的方法。而且从它开始的embedding的概念，使我们在NLP研究方面，也终于可以开始站在巨人的肩膀上了。</p>
<p>它使用的方式就是embedding，将词用向量来表示。如下：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/word2vecSample.png" class="" title="CF公式">
<p>每一列是一个词的向量表示，向量中的每个值可以看做是一个特征，比如king和queen在性别和皇室上都非常突出，这是非常符合直觉的。<br>注意：上图只是为了举例，真实的embedding向量中的值基本是很难解释的。</p>
<p>要得到词的embedding,我们只需要将embedding矩阵与该词的one-hot相乘，如下：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/embedding_matrix.png" class="" title="CF公式">

<p>这个embedding矩阵如何得到呢？</p>
<p>word2vec论文中介绍了2种方式，CBOW和skip gram。我以skip gram为例作一个简要的介绍：</p>
<p>比如我们的语料库大小为10000，现在有一个英文句子： I will try to climb a very high mountain tomorrow.<br>我们随机选择其中一个词来预测它上下文的其它词，比如 climb，接着我们就可以在它一定的上下文范围内选取词作为样本，如：(climb,mountain),(climb, high),(climb, I),(climb,to)。</p>
<p>接着我们使用这些选取的词的one-hot来乘以一个Embedding矩阵 E得到其词向量，再通过一层神经网络+softmax得到一个10000维的向量，向量的各个值就代表每个词出现在其上下文中的概率。<br>即： Oc * E = ec -&gt; θT * ec -&gt; softmax -&gt; yhat<br>其中E和θ就是我们的待训练参数，直接使用优化器进行更新，便可以得到我们的E。</p>
<p>而CBOW则差不多，区别在于CBOW是用多个词来预测一个词。</p>
<p>当然，word2vec还有很多的细节，比如如何通过分层计算或者负采样来减少softmax层的计算量以及选取上下文词的启发式方法等。你可以在文末找到该论文进行翻阅。</p>
<p>而item2vec 就是把item当作词，一起出现的item当作上下文(比如用户浏览的item的集合)，使用上面的word2vec方式学习到item的embedding矩阵，有了它，再用任何向量近似算法计算向量相似性作为item相似性还不是任你施为~ </p>
<p>比如根据用户最近浏览过的item推荐与之相似的item。</p>
<p><strong>和CF的区别</strong><br>都是计算隐相量embedding，item2vec和上面介绍的MF主要的区别就是MF计算的embedding是user-item的，而item2vec计算出的embeeding是item-item的，从二者最终的使用方式上便可看出来。其次，二者计算的方式也完全不一样~</p>
<p>至于二者的效果，MF更容易推荐比较热门的内容，而item2vec在时间窗口的基础上更能推荐user最近浏览的相似item。</p>
<h3 id="基于内容"><a href="#基于内容" class="headerlink" title="基于内容"></a>基于内容</h3><p>基于内容的推荐方法最好解释，它的思路非常简单，就是建立在“用户经常看什么，我们就给他推荐什么”的思路上。它应该是诞生最早的推荐方法了。<br>这种方法的一个特点是它的独立性，即给某个用户进行推荐的策略只跟这个用户有关，而与其它用 户的行为无关。</p>
<p>缺点也比较明显，没什么扩展性，且需要已经有一些用户的行为历史。</p>
<p>算法的主要流程为：</p>
<ol>
<li>给所有的item做分类，或者打标签，早期通过手动或者提取关键字来做，而后可以通过上面介绍的embeding方法来计算相似性来做，或者SVD。</li>
<li>做用户画像，基于用户的长短期行为得到用户感兴趣的分类或者标签。</li>
<li>基于1，2的结果进行推荐</li>
</ol>
<p>召回方法还有不少，上面介绍的几个只是比较有代表性的几个算法，使用这些算法进行多路召回并合并结果，得到的新的集合，就是我们下一步，排序的输入了。</p>
<h2 id="排序算法"><a href="#排序算法" class="headerlink" title="排序算法"></a>排序算法</h2><p>如果说召回决定了我们推荐效果的天花板，那么排序就决定了我们最终逼近天花板的程度。</p>
<p>在文章开头说过，排序阶段一般分成三个步骤：粗排-&gt;主排序-&gt;重排序。</p>
<p>粗排的原因时因为排序算法一般使用较为复杂的模型，使用较多的参数，速度相对较慢，如果召回阶段产出的item过多，会导致排序时间过长。于是先用一次粗排的过程来缩小样本范围。因此粗排一般使用比较简单的排序方法，比如使用后验CTR(点击率预估)和入库时的预估CTR值直接排序。</p>
<p>主排序则是我们要介绍的学习排序。</p>
<p>重排则是对主排的结果进行一些筛选，比如把结果放到一个类似于session model或者强化学习的模型里面进行重新排序，主要突出用户最近行为的特点。一般来说使用更少的样本范围，比如只把主排序结果的top k进行重排。</p>
<p>所以，最影响排序结果的，还是主排序部分。我们这里介绍的方法也是主排序的方法。</p>
<h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>逻辑回归(logistic regression)这里就不展开来讲了，简单的说就是用函数关系来拟合真实的分布，然后用一个非线性的转换函数将结果拟合成分类结果。</p>
<p>推荐系统中基于一阶特征的逻辑回归如下：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/LR_formula.png" class="" title="LR公式">
<p>上式的x1,x2,…,xn代表不同的特征，sigmoid是一个可以将函数值映射到0-1间的函数：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/sigmoid.png" class="" title="sigmoid公式">
<p>接着再用交叉熵定义损失函数并用梯度下降更新求得w即可。</p>
<p>需要注意的是样本数据的选择和清洗，比如明显的异常数据便应该去掉。<br>其次，在特征的选取上，那种只有少量数据才拥有的特征意义就不大。</p>
<p>逻辑回归的缺点在于，要想取得好的结果，人工组合的特征不能少，但是人工特征需要不断组合、测试、调优，非常耗费人力，我们能否在模型层面自行进行特征组合呢？</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>单独写一篇文章来讲GBDT。<br>链接：todo..</p>
<h3 id="GBDT-LR"><a href="#GBDT-LR" class="headerlink" title="GBDT + LR"></a>GBDT + LR</h3><p>这个思路来自于《practical lessons from predicting clikcks on ads at facebook》这篇论文。</p>
<p>一句话就可以概括论文的主要想法：<br>逻辑回归进行融合特征时，一需要手动组合，二调参麻烦，而GBDT这种提升树模型不断用新的决策树学习残差的过程，就相当于不断地把特征变幻成了新的特征，如果把这种高维特征再放入LR模型中去训练，能不能得到更好的结果呢？</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/gbdt&lr.png" class="" title="gbdt&amp;lr-model">
<p>做法也很好实现，先训练好GBDT模型，再把每颗决策树的结果作为新的分类特征，然后使用LR模型进行训练。</p>
<p>看起来有一些trick，但实际效果确实在很多情况下略优于二者。<br>但2个模型并不是联合训练而是单独进行训练的，二者优化目标不同，从而解释性也就弱了。</p>
<h3 id="FM-factor-machine"><a href="#FM-factor-machine" class="headerlink" title="FM(factor machine)"></a>FM(factor machine)</h3><p>由上面的LR模型，我们有了一阶线性模型：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/state-1-LR.png" class="" title="lr公式">

<p>其实很容易想到，想要融入特征组合的概念，我们只需要添加一项：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/state-2-feature-combine.png" class="" title="特征组合">
<p>将特征两两组合构成新特征，再次放入线性模型，似乎就在模型层面完成了特征组合了。</p>
<p>其实不然，上诉方法在现实中是很难运用的，因为现实中的数据特征往往非常多，如淘宝京东的商品特征，量级超过千万。<br>这种情况下，数据矩阵是高度稀疏的，xi,xj同时不为0的可能性非常小，从而使得wij的训练几乎不可能。</p>
<p>FM模型就是一种求解这种高阶稀疏矩阵的方式，它将wij转化成2个向量大小为k的一维向量vi和vj的內积</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/fm-formula.png" class="" title="fm公式">
<p>它的本质还是我们上面提到的embedding，为什么它能解决稀疏矩阵的计算问题呢？<br>因为它的计算并不依赖于xi,xj这种特征组合是否出现，vi的本质是一个embedding, 于某个特征xi而言，只要有足够多xi和其它任何特征一起出现的样本，那么vi就是可以被训练出来的。此时和同样被训练出来的vj计算內积，便可以得到二者组合特征的权重。如下：</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/fm-illustrate.jpg" class="" title="fm图解">
<p>这就是embedding的核心特点，将one-hot或者1-0的硬匹配，转换成了向量间的软匹配，从而能够近似的得到2个本来匹配不上的特征的关系。</p>
<p><strong>公式化简</strong><br>上诉FM公式中的xixj交叉项是可以化简的，从而更好的进行model serving。</p>
<p>首先，我们要考虑的交叉项肯定不包括自己与自己组合，所以xixi这种情况不考虑，其次xixj,xjxi这种重复的，我们也只算一次。那么原始可以化简如下:</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/FM-briefy.png" class="" title="fm化简">
<ol>
<li>第一步式子中的1/2就是在去重，而减去的项则是自身与自身进行的特征组合。</li>
<li>第二步则是将向量內积展开，k是向量v的维度。</li>
<li>第三步则是先将k求和移到最外层，然后内层是在确定某个特征的第l位的情况下，与其它特征对应的第l位相乘并求和。</li>
<li>第三步到第四步，注意2个求和符号，虽然一个是i，一个是j，但范围相同，求和的对象也相同，所以实际是一样的，直接把乘改为平方即可。</li>
</ol>
<p>在实际的编码中就很好表示了，Σvilxi就是 样本特征矩阵*v, 最外层的Σ则是求和操作。</p>
<p>再根据实际情况是分类还是回归选择合适的损失函数进行求导即可，因为我们的式子里有常数项、一阶项和二阶交叉项，所以导数也是3个哦~</p>
<h3 id="使用DNN"><a href="#使用DNN" class="headerlink" title="使用DNN"></a>使用DNN</h3><p>深度学习在图像和NLP领域都搞得热火朝天如火如荼，那推荐系统可以蹭一蹭吗？答案是可以的，比起上诉的LR,GBDT,FM，神经网络模型最大的优点就是它可以完全自动地对特征进行非线性的组合，且覆盖了二阶组合和高阶组合。</p>
<p>将DNN引入推荐系统，最常用的方法就是wide and deep模型，它来自于google的一篇论文：《Wide &amp; Deep Learning for Recommender Systems》(文末附链接)。</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/wideAndDeep.png" class="" title="wideAndDeep模型">
<p>左边的wide部分是一个LR或者FM模型，而右边的deep则是一个神经网络模型，或者说MLP模型。w&amp;d模型的输出是将wide侧输出与deep侧最后一个隐层的输出相加，然后再进行激活得到的。这样做的目的是可以在一次反向传播中同时更新两边的参数，达到联合训练的目的。</p>
<img src="/2020/05/01/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A6%82%E8%A7%88/w&d-formula.png" class="" title="w&amp;d激活">
<p>x_cross是组合特征。<br>Loss目标函数则根据项目的实际情况自行定义，定义好后，再利用神经网络的反向传播进行求解即可。</p>
<p>当然，DNN也有其缺点。而且成也萧何败萧何，上面我们说自动组合特征是DNN的优点，但也是缺点。这种最纯粹的w&amp;d模型，只依靠MLP本身来自动对特征进行组合，但其内里却完全是一个黑盒子，我们并不知道真正组合了什么。</p>
<p>一般来说在实际进行使用的时候，这些排序模型在大规模数据上得到的效果是 w&amp;d&gt;LR+GBDT&gt;GBDT&gt;LR的。<br>当然，实际情况实际讨论，在模型落地的时候我们都会进行一些符合业务逻辑的修改，或者加入一些其它的想法。所以，选择合适的才是最重要的。</p>
<p>其次，对于实际上线的效果，有很多评估指标，通用的比如AUC、F1，测试集表现等。不同的业务还能根据实际业务定义相应的业务指标。</p>
<p>在特征的选取上，排序阶段则是可以尽量把相关的side info都特征化，毕竟这个阶段的目标就是精确，而更多的信息往往能够得到更好的结果。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>再总结一下流程，首先，使用多路召回并合并，从所有item中找到最适合推荐给用户的近千的item。进入排序阶段后，如果item过多导致排序时间长，可以加入粗排阶段，用一些简单的排序模型对召回返回的item进行一次筛选，进一步缩小item范围。接着再使用精确度较高的主排序模型进行精准排序。最后，再根据一些业务策略筛选，比如使实际推荐结果多样化，或者去除已读item等，然后再推荐给用户。</p>
<p>而为了验证模型效果和继续优化模型，我们要继续收集用户的各种行为和反馈。</p>
<p>这些行为和反馈一部分可以实时的用来更新在线推荐模型，让用户的实时行为在下一次刷新中即可得到体现。<br>而所有这些数据都应该被记录下来补充进我们的离线训练数据，从而用更大的模型进行离线训练，从而周期性地对模型进行更新。</p>
<h2 id="reference"><a href="#reference" class="headerlink" title="reference"></a>reference</h2><p><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Andrew Ng在coursera上的机器学习课程</a><br><a href="https://arxiv.org/abs/1301.3781v3" target="_blank" rel="noopener">word2vec论文:Efficient Estimation of Word Representations in Vector Space</a><br><a href="https://arxiv.org/abs/1603.04259" target="_blank" rel="noopener">item2vec论文:Item2Vec: Neural Item Embedding for Collaborative Filtering</a><br><a href="https://zhuanlan.zhihu.com/p/58160982" target="_blank" rel="noopener">知乎张俊林专栏：全能的FM模型</a><br><a href="https://quinonero.net/Publications/predicting-clicks-facebook.pdf" target="_blank" rel="noopener">gbdt&amp;lr论文：Practical Lessons from Predicting Clicks on Ads at Facebook</a><br><a href="https://arxiv.org/abs/1606.07792" target="_blank" rel="noopener">wide and deep论文:Wide &amp; Deep Learning for Recommender Systems</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/04/21/%E7%AE%80%E5%8D%95%E7%9A%84%E8%A7%A6%E5%8F%91%E8%AF%8D%E8%AF%86%E5%88%AB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/21/%E7%AE%80%E5%8D%95%E7%9A%84%E8%A7%A6%E5%8F%91%E8%AF%8D%E8%AF%86%E5%88%AB/" class="post-title-link" itemprop="url">简单的触发词识别</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-21 14:01:49" itemprop="dateCreated datePublished" datetime="2020-04-21T14:01:49+08:00">2020-04-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-10 17:37:52" itemprop="dateModified" datetime="2020-05-10T17:37:52+08:00">2020-05-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本文参考andrew Ng, Sequence Model,  notebook-Trigger word detection.</p>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>当你对苹果手机叫出”hey siri”，或对小米手机叫出”小爱同学”时，手机助手会立刻出现，这就是触发词识别系统，是一种识别音频并且在接收到某个触发词的时候激活的程序。不同于一般的语音识别需要大量的数据(超过10W小时)来训练，触发词系统的训练相对简单很多。</p>
<p>当你完成这个程序的时候，你可以将它扩展并布置到自己的电脑上，每当你说出某个激活词的时候，你的电脑可以随即自动打开某个app，或者是播放某一首音乐，听起来是不是很棒(吧？)。</p>
<p>因为声音数据是序列数据，我们用RNN&amp;GRU来做这个用识别触发词的程序，当程序听见语音”激活”的时候就给出一个某种反应。</p>
<h2 id="创建语音数据集"><a href="#创建语音数据集" class="headerlink" title="创建语音数据集"></a>创建语音数据集</h2><p>语音数据是很难获取的一类数据，这里我们需要的样本是10秒钟的数据，其中会随时出现我们的激活词。如果我们自己录制这些样本，因为数量比较大，这将会相当困难，于是我们考虑使用音频合成来制作我们的数据集。<br>首先，我们需要一些用于合成的元数据。因此我们需要去到各种环境中用不同人，不同口音录制”激活”以及任何其它语音。<br>将它们分为 positive\negative\background三类，positive为不同人不同口音念我们的激活词， negative 则是随机的其它词，background则是各种背景音，持续10秒。<br>PS：因为口音和词语长短不同，positive和negative的语音文件长度不定。</p>
<p>收集完语音后，我们需要对其进行处理。<br>一般来说，录制的声音文件根据设备的不同，拥有不同的HZ数。这里假设我们采集的样本为44100HZ，即录制的音频文件每秒有44100个数字。</p>
<p>要直接处理这种文件相对来说是较为困难的，所以更流行的做法是将数据用傅里叶变换转化为频谱(spectrogram)，这是信号处理相关的知识。这里我们直接用软件包实现。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; graph_spectrogram(&quot;audios&#x2F;train_example.wav&quot;)</span><br></pre></td></tr></table></figure>
<img src="/2020/04/21/%E7%AE%80%E5%8D%95%E7%9A%84%E8%A7%A6%E5%8F%91%E8%AF%8D%E8%AF%86%E5%88%AB/spectrogram-img.png" class="" title="频谱片">

<p>频谱图直观上体现的是频率(y轴)和时间(x轴)的图像关系，偏绿的颜色表示频率较高，而蓝色则相反。</p>
<p>频谱输出的数据维度是由程序的超参数和输入数据的长度决定的，这里我们使用的转换程序，10秒的数据将拥有5511个timestep，即Tx=5511.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;spectrogram shape:&quot;, x.shape)</span><br></pre></td></tr></table></figure>
<p>输出为(101,5511)。<br>因此我们可以定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Tx &#x3D; 5511 </span><br><span class="line">n_freq &#x3D; 101 # 每个timestep中输入到模型中的频率数</span><br><span class="line">Ty &#x3D; 1375 # 我们程序的输出将会把10秒切分为1375份</span><br></pre></td></tr></table></figure>

<h3 id="合成语音"><a href="#合成语音" class="headerlink" title="合成语音"></a>合成语音</h3><p>三个步骤</p>
<ol>
<li>首先我们随机选取一个background音频</li>
<li>随机将0-4个正样本音频片段插入</li>
<li>随机将0-2个负样本音频片段插入<br>因为是我们控制的插入，我们同时可以得到Ylabel数据，即第几个timestep为激活词刚刚说完的时刻，记为y&lt;t&gt;=1,但一般来说为了避免label数据过于稀疏，我们会把激活词结束后的一段timestep的y都置为1，这里我们选择的一段为50个timestep。</li>
</ol>
<p>使用pydub包来处理音频，这个包将会用ms作为时间轴最小单位，10秒的数据将会有10000个timestep.</p>
<p>注意，合成后的音频应该依然为10秒，即正负样本都应该被background完全容纳，且二者互相不能重叠。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activates, negatives, backgrounds &#x3D; load_raw_audio()</span><br></pre></td></tr></table></figure>
<p>先来实现几个工具方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"># 获取插入位置</span><br><span class="line">def get_random_time_segment(segment_ms):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    从10000个timestep中随机取一个timestep，作为之后的插入位置</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    segment_ms -- 要插入音频的ms长度</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    segment_time -- a tuple(segment_start, segment_end) in ms，插入的开始和结束timestemp</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    segment_start &#x3D; np.random.randint(low&#x3D;0, high&#x3D;10000-segment_ms)   # 确保插入的音频不会超出background的结尾</span><br><span class="line">    segment_end &#x3D; segment_start + segment_ms - 1</span><br><span class="line">    </span><br><span class="line">    return (segment_start, segment_end)</span><br><span class="line"></span><br><span class="line"># 检查是否发生重叠</span><br><span class="line">def is_overlapping(segment_time, previous_segments):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    segment_time -- a tuple(segment_start, segment_end) 新插入片段的起止时间</span><br><span class="line">    previous_segments -- a list of tuples(segment_start, segment_end) 已经插入的片段的起止时间列表</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    True or False 代表是否发生重叠</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    segment_start, segment_end &#x3D; segment_time</span><br><span class="line"></span><br><span class="line">    overlap &#x3D; False</span><br><span class="line">    </span><br><span class="line">    for previous_start, previous_end in previous_segments:</span><br><span class="line">        # 作业提示可以用 if ... &lt;&#x3D; ... and ... &gt;&#x3D; ... 这种形式实现，我没想出来呢</span><br><span class="line">        if (previous_start &gt; segment_end or segment_start &gt; previous_end) &#x3D;&#x3D; False :</span><br><span class="line">            overlap &#x3D; True</span><br><span class="line"></span><br><span class="line">    return overlap</span><br><span class="line"></span><br><span class="line"># 向background中随机位置插入音频片段，确保不会发生重叠和超出</span><br><span class="line">def insert_audio_clip(background, audio_clip, previous_segments):</span><br><span class="line">    &quot;&quot;&quot; </span><br><span class="line">    Arguments:</span><br><span class="line">    background -- 10秒钟的背景音频.  </span><br><span class="line">    audio_clip -- 要插入的音频. </span><br><span class="line">    previous_segments -- 已经插入在background中的音频的起止时间</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    new_background -- 插入音频后的新的background</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Get the duration of the audio clip in ms</span><br><span class="line">    segment_ms &#x3D; len(audio_clip)</span><br><span class="line">    </span><br><span class="line">    segment_time &#x3D; get_random_time_segment(segment_ms)</span><br><span class="line">    </span><br><span class="line">    while is_overlapping(segment_time, previous_segments):</span><br><span class="line">        segment_time &#x3D; get_random_time_segment(segment_ms)</span><br><span class="line"></span><br><span class="line">    previous_segments.append(segment_time)</span><br><span class="line">    </span><br><span class="line">    # 将音频在background的对应位置叠加进去</span><br><span class="line">    new_background &#x3D; background.overlay(audio_clip, position &#x3D; segment_time[0])</span><br><span class="line">    </span><br><span class="line">    return new_background, segment_time</span><br></pre></td></tr></table></figure>

<p>插入了激活词语音后，我们就需要对Ylabel做出更新，如我们上面所说，将结束位置的后面50个y都更新为1。<br>因为我们的y的维度为1375而音频数据维度为10000，所以要记得做一下缩放。还要确保如果音频插入在background的尾部，导致后面没剩下50个值，注意不要越界。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def insert_ones(y, segment_end_ms):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Arguments:</span><br><span class="line">    y -- numpy array of shape (1, Ty),训练样本的标签值</span><br><span class="line">    segment_end_ms -- 插入的激活音频的结束时间</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    y -- 更新后的 labels</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # 缩放</span><br><span class="line">    segment_end_y &#x3D; int(segment_end_ms * Ty &#x2F; 10000.0)</span><br><span class="line">    </span><br><span class="line">    for i in range(segment_end_y, segment_end_y + 50):</span><br><span class="line">        if i &lt; Ty:</span><br><span class="line">            y[0, i] &#x3D; 1</span><br><span class="line">    </span><br><span class="line">    return y</span><br></pre></td></tr></table></figure>

<p>接下来就可以创建我们的训练样本了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">def create_training_example(background, activates, negatives):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    Creates a training example with a given background, activates, and negatives.</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    background -- a 10 second background audio recording</span><br><span class="line">    activates -- a list of audio segments of the word &quot;activate&quot;</span><br><span class="line">    negatives -- a list of audio segments of random words that are not &quot;activate&quot;</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    x -- the spectrogram of the training example</span><br><span class="line">    y -- the label at each time step of the spectrogram</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # 减小噪音</span><br><span class="line">    background &#x3D; background - 20</span><br><span class="line"></span><br><span class="line">    y &#x3D; np.zeros((1,Ty))</span><br><span class="line"></span><br><span class="line">    previous_segments &#x3D; []</span><br><span class="line">    </span><br><span class="line">    # 随机插入0-4段激活音频</span><br><span class="line">    number_of_activates &#x3D; np.random.randint(0, 5)</span><br><span class="line">    random_indices &#x3D; np.random.randint(len(activates), size&#x3D;number_of_activates)</span><br><span class="line">    random_activates &#x3D; [activates[i] for i in random_indices]</span><br><span class="line">    </span><br><span class="line">    for random_activate in random_activates:</span><br><span class="line">        background, segment_time &#x3D; insert_audio_clip(background, random_activate, previous_segments)</span><br><span class="line">        segment_start, segment_end &#x3D; segment_time</span><br><span class="line">        y &#x3D; insert_ones(y, segment_end)</span><br><span class="line"></span><br><span class="line">    # 随机插入0-2段negative语音</span><br><span class="line">    number_of_negatives &#x3D; np.random.randint(0, 3)</span><br><span class="line">    random_indices &#x3D; np.random.randint(len(negatives), size&#x3D;number_of_negatives)</span><br><span class="line">    random_negatives &#x3D; [negatives[i] for i in random_indices]</span><br><span class="line"></span><br><span class="line">    for random_negative in random_negatives:</span><br><span class="line">        background, _ &#x3D; insert_audio_clip(background, random_negative, previous_segments)</span><br><span class="line">    </span><br><span class="line">    # 标准化一波 </span><br><span class="line">    background &#x3D; match_target_amplitude(background, -20.0)</span><br><span class="line"></span><br><span class="line">    # 导出训练样本</span><br><span class="line">    file_handle &#x3D; background.export(&quot;train&quot; + &quot;.wav&quot;, format&#x3D;&quot;wav&quot;)</span><br><span class="line">    print(&quot;File (train.wav) was saved in your directory.&quot;)</span><br><span class="line">    </span><br><span class="line">    # 绘制频谱图</span><br><span class="line">    x &#x3D; graph_spectrogram(&quot;train.wav&quot;)</span><br><span class="line">    </span><br><span class="line">    return x, y</span><br></pre></td></tr></table></figure>

<p>然后我们就用这个方法去创建一大堆的训练样本。<br>然后将创建好的训练样本赋值给 <strong><em>X</em></strong> 和 <strong><em>Y</em></strong></p>
<h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>我们用keres来实现这个模型。<br>模型共四层</p>
<ul>
<li>第一层是一个卷积层，因为我们的频谱数据为1维，所以是一维卷积(196个滤波器，滤波器长度15，步幅为4)，卷积后再做一次BN，并用ReLu激活，再用Dropout进行一次正则化。</li>
<li>第二层是一个GRU层，保证网络的记忆能力，并且也对输出的数据做Dropout和BN处理。</li>
<li>第三层继续用一个GRU层。</li>
<li>最后一层为全连接，且接sofxmax作为输出。</li>
</ul>
<p>如下：</p>
<img src="/2020/04/21/%E7%AE%80%E5%8D%95%E7%9A%84%E8%A7%A6%E5%8F%91%E8%AF%8D%E8%AF%86%E5%88%AB/model.png" class="" title="模型图片">

<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">def model(input_shape):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    用keras构造训练模型</span><br><span class="line">    </span><br><span class="line">    Argument:</span><br><span class="line">    input_shape -- 模型输入数据的形状</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">    model -- keras model 实例</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    X_input &#x3D; Input(shape &#x3D; input_shape)</span><br><span class="line">    </span><br><span class="line">    # Layer 1: CONV layer </span><br><span class="line">    X &#x3D; Conv1D(filters&#x3D;196,kernel_size&#x3D;15,strides&#x3D;4)(X_input)   # CONV1D</span><br><span class="line">    X &#x3D; BatchNormalization()(X)                                 # Batch normalization</span><br><span class="line">    X &#x3D; Activation(&quot;relu&quot;)(X)                                   # ReLu activation</span><br><span class="line">    X &#x3D; Dropout(rate&#x3D;0.8)(X)                                    # dropout (use 0.8)</span><br><span class="line"></span><br><span class="line">    # Layer 2: First GRU Layer</span><br><span class="line">    X &#x3D; GRU(units&#x3D;128, return_sequences &#x3D; True)(X)           # GRU (use 128 units and return the sequences)</span><br><span class="line">    X &#x3D; Dropout(rate&#x3D;0.8)(X)                                 # dropout (use 0.8)</span><br><span class="line">    X &#x3D; BatchNormalization()(X)                              # Batch normalization</span><br><span class="line">    </span><br><span class="line">    # Layer 3: Second GRU Layer</span><br><span class="line">    X &#x3D; GRU(units&#x3D;128, return_sequences &#x3D; True)(X)           # GRU (use 128 units and return the sequences)</span><br><span class="line">    X &#x3D; Dropout(rate&#x3D;0.8)(X)                                 # dropout (use 0.8)</span><br><span class="line">    X &#x3D; BatchNormalization()(X)                              # Batch normalization</span><br><span class="line">    X &#x3D; Dropout(rate&#x3D;0.8)(X)                                 # dropout (use 0.8)</span><br><span class="line">    </span><br><span class="line">    # Layer 4: Time-distributed dense layer</span><br><span class="line">    X &#x3D; TimeDistributed(Dense(1, activation &#x3D; &quot;sigmoid&quot;))(X) # time distributed  (sigmoid)</span><br><span class="line"></span><br><span class="line">    model &#x3D; Model(inputs &#x3D; X_input, outputs &#x3D; X)</span><br><span class="line">    </span><br><span class="line">    return model</span><br></pre></td></tr></table></figure>
<p>注意上面最后一层的 TimeDistributed 层，<a href="https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/" target="_blank" rel="noopener">这里</a>有关于它的使用的完全解释。<br>简单的说，它是使用timestep来进行操作，且对每个timestep的数据都共享权重。<br>比如这里我们用它包装了Dense，即是说对每个timestep的结果，都使用同样的参数计算Dense输出结果。</p>
<p>接下来就可以进行训练了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">opt &#x3D; Adam(lr&#x3D;0.0001, beta_1&#x3D;0.9, beta_2&#x3D;0.999, decay&#x3D;0.01)</span><br><span class="line">model.compile(loss&#x3D;&#39;binary_crossentropy&#39;, optimizer&#x3D;opt, metrics&#x3D;[&quot;accuracy&quot;])</span><br><span class="line">model.fit(X, Y, batch_size &#x3D; 5, epochs&#x3D;100)</span><br></pre></td></tr></table></figure>
<p>得到：<br>Epoch 100/100<br>26/26 [==============================] - 34s - loss: 0.0610 - acc: 0.9796 </p>
<p>接下来如果有开发集，可以对模型进行测试，开发集样本最好来自真实分布，即去真实的环境说话进行录音。</p>
<h2 id="预测和更多操作"><a href="#预测和更多操作" class="headerlink" title="预测和更多操作"></a>预测和更多操作</h2><p>接下来就可以用这个训练好的模型进行预测了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x &#x3D; graph_spectrogram(filename)</span><br><span class="line">x  &#x3D; x.swapaxes(0,1)</span><br><span class="line">x &#x3D; np.expand_dims(x, axis&#x3D;0)</span><br><span class="line">predictions &#x3D; model.predict(x)</span><br></pre></td></tr></table></figure>

<p>还可以在触发的时候做更多的操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">consecutive_timesteps &#x3D; 0</span><br><span class="line">for i in range(Ty):</span><br><span class="line">        consecutive_timesteps +&#x3D; 1</span><br><span class="line">        if predictions[0,i,0] &gt; threshold:</span><br><span class="line">            # 任何操作</span><br><span class="line">            whatever_you_wanna_do()</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/" class="post-title-link" itemprop="url">机器学习中的各种优化器算法</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-16 00:20:14 / 修改时间：00:40:42" itemprop="dateCreated datePublished" datetime="2020-04-16T00:20:14+08:00">2020-04-16</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E7%AE%97%E6%B3%95/" itemprop="url" rel="index"><span itemprop="name">基础算法</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>深度学习中的优化器有很多种，除了我们熟悉的梯度下降外，还有一些诸如 RMSProp，adam 等优秀的优化器。来了解一波~</p>
<h2 id="梯度下降-GD-BGD-MBGD"><a href="#梯度下降-GD-BGD-MBGD" class="headerlink" title="梯度下降 GD,BGD,MBGD"></a>梯度下降 GD,BGD,MBGD</h2><p>梯度下降（GD：gradient descent）大家都很熟悉，这里也不做详细介绍。整体就是先初始化求解参数，然后通过求解损失函数对求解参数的导数来对我们的求解参数进行更新，直到收敛。 </p>
<p>梯度下降分为BGD（batch：批量梯度下降），SGD（stochastic：随机梯度下降）和 MBGD(Mini-Batch：小批量梯度下降)，区别在于每次更新梯度时使用的样本的数量，分别为全部样本，单个样本和一部分样本。</p>
<p>梯度下降找到的最优解一般为函数的一个鞍点，即局部最优解。<br>MBGD和SGD因为样本较少，随机性太强，梯度往往震荡很大，如下：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/GDvsMBGD.png" class="" title="GDvsMBGD">

<p>要解决这个问题，使用MBGD进行优化时我们可以对学习率进行衰减来使之收敛。</p>
<p>PS:因为计算机本身的一些性质，将批次量设置为2的幂数计算会更快。</p>
<p>当然，还有比小批次下降更快的算法。但在学习它们之前，我们首先要了解指数加权平均。</p>
<h2 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h2><p>什么是指数加权平均？</p>
<p>参考吴恩达老师对此的讲解，用一个例子来进行说明：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/LondonTempreture.png" class="" title="LondonTempreture">

<p>上图是伦敦一年之中每天的温度情况，我们来对它做一些处理，把每天的温度值记作Vn。则：<br>V0=0， V1 = 0.9*V0 + 0.1*θ1， V2=0.9V1 + 0.1*θ2，…<br>θ为上图中当天真实的温度值，这个0.9我们记作β，那么公式记为：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/formulaVt.png" class="" title="formulaVt">

<p>稍微进行一下联想，这个V可以近似看做是之前1/(1-β)天的平均值，当我们分别取β=0.9(红色曲线)和0.98(绿色曲线)时，图像为：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/TempretureHandled.png" class="" title="TempretureHandled">

<p>β取0.9，我们把这个式子展开：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/UnfoldFormulaV.png" class="" title="UnfoldFormulaV">

<p>θ随着时间推后是β的指数级衰减，且一般来说，指数加权的衰减大约会在1/(1-β)后衰减到大约三分之一的程度，比如0.9的1/(1-0.9)次方约等于0.35，所以我们说它大约是最近10天的平均值。这就是指数加权平均名称的由来。</p>
<p>指数加权平均减少了存储空间的使用，当我们需要某个V的值时，只需要通过计算即可获得，因此它在机器学习中得到了大量的应用。</p>
<p>细心的你可能注意到，我们的V0取值为0，这会在计算初期的时候产生较大的误差值。<br>因此在很多时候，我们会对V的值进行<strong><em>偏差修正</em></strong>。<br>使 Vt = Vt/(1-β^t)  ，从而对Vt进行放大，而随着t的增大，放大率会逐渐趋于1。</p>
<p>有了这个基础，我们就可以介绍一些其它的优化器方法。</p>
<h2 id="动量梯度算法（momentum）"><a href="#动量梯度算法（momentum）" class="headerlink" title="动量梯度算法（momentum）"></a>动量梯度算法（momentum）</h2><p>算法的主要思想其实很简单，就是把我们刚才学习的指数加权平均用来计算梯度，然后用计算得到的梯度来进行参数更新。<br>在上面的梯度下降算法中，考虑多维度的情况，如下：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/momentumExample.png" class="" title="momentumExample">

<p>于收敛来说，竖直方向的震荡显然是无益的，我们希望能够减小竖直方向的震荡。</p>
<p>动量梯度下降的步骤如下<br>第t次迭代：</p>
<ol>
<li>用当前小批量样本 计算 参数W和 偏差b的导数</li>
<li>Vdw = βVdw + (1-β)dw， Vdb = βVdb + (1-β)db</li>
<li>W := W - αVdw,   b := b-αVdb </li>
</ol>
<p>end<br>我们还是对批次数量进行迭代，整个算法中中有2个超参数，α和β，α为学习率，β是我们上面学习到的指数加权。</p>
<p>为什么咋这么做可以有效呢？因为指数加权平均的平均，它就让垂直方向上相反的震荡被平均从而变小，而水平方向的震荡方向是相同的，平均值依然很大，故整体收敛速度就加快了。</p>
<p>PS:动量梯度下降时基本不用偏差修正，β基本都是选择0.9。</p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>RMSprop全称为均方根传递(Root Mean Square prop), 它也可以加速梯度下降。<br>计算方式如下：</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/RMSPropStep.png" class="" title="RMSPropStep">

<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/RMSPropImg.png" class="" title="RMSPropImg">
<p>RMSprop主要的思想是缩小大的震荡，加快小的震荡。如上图，垂直方向的震荡很大，而水平方向很小，RMSprop就会缩小垂直方向的震荡，加快水平方向的速度。<br>原理很简单，如果垂直方向的震荡较大，那么Sdb就会很大，那么作为除数，更新速度就减慢了，相反，水平方向较慢，Sdw就较小，W的更新速度就加快了。<br>当然，实际应用中，并没有垂直水平这么简单，我们加快的是慢的维度，减慢的是快的维度。<br>而且为了防止除0发生，通常在分母上我们会加一个很小的EPSON，大概10e-8</p>
<p>接下来，我们把RMSprop和动量结合起来，会得到一个更好的优化算法。<br>为了防止冲突，RMSprop中的β，我们用β_2表示</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>adaptive moment estimation 自适应矩估计<br>adam优化算法实际上就是将RMSprop和动量梯度下降结合起来的算法。<br>在机器学习领域和深度学习领域中，曾经提出了非常多的优化算法，但大多数算法都不能很好的适应于不同的网络结构，adam算法是少有的在非常多网络结构中都能够产生非常好效果的算法。</p>
<p>在机器学习领域和深度学习领域中，曾经提出了非常多的优化算法，比如Adagrad,Adadelta等，但大多数算法都不能很好的适应于不同的网络结构，adam算法是少有的在非常多网络结构中都能够产生非常好效果的算法。</p>
<p>初始化Vdw，Sdw,  Vdb, Sdb为0</p>
<img src="/2020/04/16/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%90%84%E7%A7%8D%E4%BC%98%E5%8C%96%E5%99%A8%E7%AE%97%E6%B3%95/AdamStep.png" class="" title="AdamStep">

<p>l为网络层数，两个β分别是动量梯度和RMSprop中的加权，α为学习率，同样，为了防止除0发生，通常在分母上我们会加一个很小的EPSON，大概10e-8（上图没加）。</p>
<p>几个超参数，一般β1选择0.9，β2选择0.999，EPSON选择10e-8。<br>而α，一个好的方法就是逐渐减小学习速率，使用一个衰减率来对学习率进行衰减。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/04/03/BERT%E7%AC%94%E8%AE%B0-%E7%AE%80%E4%BB%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/03/BERT%E7%AC%94%E8%AE%B0-%E7%AE%80%E4%BB%8B/" class="post-title-link" itemprop="url">BERT笔记-简介</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-03 13:48:42" itemprop="dateCreated datePublished" datetime="2020-04-03T13:48:42+08:00">2020-04-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-05-05 23:04:37" itemprop="dateModified" datetime="2020-05-05T23:04:37+08:00">2020-05-05</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>BERT，全名 Bidirectional Encoder Representation from Transformers。是Google AI团队在2018年推出的一个NLP模型，在机器阅读理解顶级水平测试SQuAD1.1上全面超越人类表现，并在11项NLP测试中拿到最佳成绩。这也导致BERT的大火。</p>
<p>BERT的出现彻底改变了pre-train产生词向量和下游NLP训练任务间的关系。</p>
<p>//todo 留坑</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">Transformer模型简介(笔记)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-02 22:04:42 / 修改时间：22:21:58" itemprop="dateCreated datePublished" datetime="2020-04-02T22:04:42+08:00">2020-04-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Transformer来自Google 2017年的一篇文章，在原来的Attention&amp;RNN模型上抛弃了RNN，用全attention的结构取得了更好的效果。<br>这里做一做自己学习的笔记，也算一个简单的介绍。<br>内容图片很多来自于原论文<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a>和<a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a>这篇文章。</p>
<h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>原论文给出的结构如下：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/structureOfTransformer.png" class="" title="transformer结构">
<p>可以看到由左右两个部分，左边的Encoders和右边的Decoders组成。两边都有一个”N×”,表示各自由N个同样的结构重复N次组成，原文中是6。就是下面图中的样子。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/encodersAndDecoders.png" class="" title="展开">

<h2 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h2><p>我们来看一看Encoder部分。</p>
<p>因为是NLP的案例，所以我们首先要把我们的输入数据，即词变成词向量，这通过embedding实现，embedding后的数据作为Encoder的输入。<br>虽然有很多encoder，但embedding只用在最下面一层的encoder上，其它的encoder都是用上一层encoder的输出作为输入。</p>
<p>每个encoder都是一样的结构，都由两个子结构组成：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/EachEncoder.png" class="" title="encoder组成">

<p>self-attention的作用是，当你在处理某个具体的词时，self-attention允许你从句子中的其它位置处寻找线索，从而对当前词的理解和预测起到帮助。</p>
<h3 id="self-attention-细节"><a href="#self-attention-细节" class="headerlink" title="self-attention 细节"></a>self-attention 细节</h3><p>计算self-attention主要有以下几个步骤</p>
<p><strong>第一步</strong><br>计算self-attention的第一步是从每个输入向量中创建出3个向量(Querry,Key,Value)。他们通过把embedding分别与三个矩阵相乘得到，三个矩阵通过训练过程得到。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/transformerSelfAttentionVectors.png" class="" title="QSV">

<p>Q,S,V较之embedding的维度要小，在文中的维度是64，而embedding的维度是512。不必完全一样，但这是一种使得计算比较稳定的结构选择。</p>
<ul>
<li>Q = WQ * x</li>
<li>K = WK * x</li>
<li>V = WV * x</li>
</ul>
<p><strong>第二步</strong><br>计算self-attention的第二步是计算一个score。 假设我们正在计算的句子的第一个单词为”Thinking”。我们需要把输入的句子中的每一个词与这个词运算来得到一个score，这个score决定了我们在encode当前词的时候句子其它位置所施加的影响。</p>
<p>计算方法是把当前次的Q与要计算的词的K值进行点乘。即如果我们要计算#1位置处的self-attention，第一个我们要计算的score将是把q1和k1点乘，第二个socre则是把q1和k2进行点乘。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/SelfAttentionScore.png" class="" title="计算score">


<p><strong>第三步和第四步</strong><br>第三步和第四步是将得到的score除以8，这个8是QKV向量的维度64的平方根。这可以让梯度更加稳定(直接归一值差距较大)。当然可以不是8，这里只是一个默认值。接着将结果传递给一个softmax操作，这将把socre的值标准化，使它们都为正，且和为1。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/selfAttentionSoftmax.png" class="" title="计算权重">

<p><strong>第五步</strong><br>第五步是将每个V与第四步的结果相乘。这一步从直觉上讲是保留当前词想要关注的其它词语的完整性，同时丢掉不相关的词语(通过乘以了非常小的数)。</p>
<p><strong>第六步</strong><br>将得到的带权重的数据向量相加。这将得到self-attention层在这个位置(我们这里是第一个词)的输出。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/selfAttentionOutput.png" class="" title="encoder输出">

<p>这就是self-attentionde的计算过程，结果向量我们将传递给接下来的 feed-forward nertal network处理。<br>当然，在实际实现中，这些计算都可以通过矩阵形式的计算从而更加快速。</p>
<p><strong>self-attention的矩阵计算</strong><br>使用矩阵，第二步到第六步实际上可以在一个公式内进行计算：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/selfAttentionMatrixCalculation.png" class="" title="矩阵计算公式">



<h3 id="multi-headed"><a href="#multi-headed" class="headerlink" title="multi-headed"></a>multi-headed</h3><p>文章进一步用一个叫做”multi-headed” attention的结构增强了self-attention。它从2个方面提升了attention层的表现：<br><strong>扩张了模型关注其他位置的能力</strong><br>在我们上面的例子中，对thinking的编码就包含了句中其它位置词的影响(当然，最大的影响依然是它自己)。在解析一些有明显指向性的代词时就显得非常有用。比如“The animal didn’t cross the street because it was too tired”中的”it”指代的谁。<br><strong>给了attention层多重”表述子空间”</strong><br>这主要通过多组[WQ,WK,QV]来实现，文中使用了8组WQ,WK,QV，这些矩阵都通过随机初始化赋值。即是说我们会得到8组QKV，从而得到8个输出矩阵。每一个都是输入数据的一个表述子空间。</p>
<p>在传递给feed-forward network前，我们需要将他们处理成一个矩阵。通过将这8个矩阵堆叠起来，再与一个权重矩阵WO相乘得到。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/transformer_attention_heads_weight_matrix_o.png" class="" title="concat结果">

<p>以上大概就是 multi-headed self-attention 的内容。原文将他们放到一张图上：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/encoderTotalLook.png" class="" title="整体造型">

<h3 id="position-encoding"><a href="#position-encoding" class="headerlink" title="position encoding"></a>position encoding</h3><p>因为放弃了使用RNN，那么句子中词与词的位置关系就被忽略了，文中使用了一种position encoding的方式将位置信息补入模型中。<br>这通过给每一个input embedding加上一个vector来实现。这些vector遵循一种<strong><em>特殊的模式</em></strong>，它存储了每个词的位置信息，通过把它与embedding相加，从而把这种信息代入到后面的QKV和点乘的计算过程中。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/transformer_positional_encoding_vectors.png" class="" title="位置编码">

<p>如果我们的embedding是512维的向量，那么要加的positional encoding 向量也是512维。</p>
<p>关于位置编码，文中使用的是三角函数的形式。</p>
<p>大概说一下什么是位置编码和为什么要使用三角函数。<br>要对位置进行编码，最简单的方式莫过于直接使用单词在文本中的位置，即1，2，3，…，N。但缺点过于明显，如果文本较长，那么位置编码的大小跨度就太大了，将这样的数据加入到模型训练中，很有可能是会喧宾夺主的抢占embedding的重要性。<br>同样，将刚才的顺序除以文本长度也是不行的，如1/N,2/N,3/N,…1。<br>我们需要位置信息，其中一个重要的信息就是相对位置信息，而这种处理方式，会导致相隔同样距离的两个词，在长度不同的文本中得到的相对位置信息不一致，甚至差距较大。<br>总结之后，那么真正适合用来做位置编码的函数似乎就是 连续且有界的周期性函数。有界保证值域不会太大，周期性保证一定程度上编码的差异会摆脱文本长度的影响，而连续则保证了两个比较靠近的词不会出现差距很大的情况。</p>
<p>于是文中使用了sin和cos函数，连续而且周期稳定，值域[-1,1]。</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/PosEncodingformula.png" class="" title="位置编码公式">

<p>加入了dmodel和i两个参数，dmodel是embedding的维度，在文中就是512，用于增大位置编码的空间表现范围。i为向量的某一维度，dmodel=512，那么i就是[0,255],这样在奇偶维度分别使用sin和cos。这样就从取值范围和取值方法两个方向上增加了取值的多样性。让位置编码更加科学。</p>
<p>当然，这个函数作者应该也是通过自身的经验与不断的实验得到的。</p>
<p>PS：GOOGLE BERT中用了新的取位置信息的方法，position embedding，这是后话。</p>
<h3 id="残差网络-Residual-network-的使用"><a href="#残差网络-Residual-network-的使用" class="headerlink" title="残差网络(Residual network)的使用"></a>残差网络(Residual network)的使用</h3><p>另一个细节，就是哪里跑不掉的resNet的使用：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/resInEncoder.png" class="" title="resNet的使用">

<p>同样，在decoder中也使用到了resNet，如果是一个2个encoder和decoder的transformer，它长这样：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/resInTransformer.png" class="" title="resNet的使用2">

<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>介绍完Encoder，大多数Decoder里的组件的作用也明朗了。接下来看看他哥俩如何一起工作。</p>
<p>再贴一下模型结构图：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/structureOfTransformer.png" class="" title="transformer结构">
<p>可以看到Decoder中有一个 Encoder-Decoder Attention 层，它接受Encoder部分最后的输出作为计算attention的Key和Value，接受它下面的self-attention层的输出作为Query。</p>
<p>其次，Decoder部分的self-attention层也与Encoder中的不同，不同于Encoder中计算单词两两间的attention，Decoder中计算的是当前单词和它前面的单词的attention，同样，也要加入位置信息。</p>
<p>文章中有张非常形象的动图：</p>
<img src="/2020/04/02/Transformer%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B-%E7%AC%94%E8%AE%B0/transformerDecodingGif.gif" class="" title="transformer结构">

<p>注意在decoder中做self-attention的时候，当前输入只应该看到当前时刻以前的输出，比如在输出第二个词的时候，输入中是不应该出现第三个词的信息的。文中处理这种情况的方法是用了一个倒三角矩阵(第i行j列的元素表示第i个输入和第j个输入的attention)，将对角线右侧元素全部设置为负无穷，这样就防止了模型看到未来的信息。</p>
<h2 id="最后一层"><a href="#最后一层" class="headerlink" title="最后一层"></a>最后一层</h2><p>decoder将输出一堆floats组成的向量，将它转换成词语，就是最后一层的工作(通常是一个Linear+Softmax)。</p>
<p>Linear layer是一个简单的全连接层，将decoder的输出投射为一个比原来大很多的向量，叫做logits vector。</p>
<p>如果我们的词空间有10000个单词，那么10000就是这个logits vector的维度，向量中每个元素对应一个具体的词。接下来你就清楚了，softmax的作用是将这个logits vector的结果变成概率，概率最高的元素对应的词就是我们的输出。</p>
<h2 id="关于训练"><a href="#关于训练" class="headerlink" title="关于训练"></a>关于训练</h2><p>todo…</p>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>todo…</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://jalammar.github.io/illustrated-transformer/" target="_blank" rel="noopener">The Illustrated Transformer</a><br><a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention Is All You Need</a><br><a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank" rel="noopener">哈佛大学的pytorch版本源码</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/" class="post-title-link" itemprop="url">实现一个简单的人脸识别系统</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-03-25 15:27:57 / 修改时间：16:36:29" itemprop="dateCreated datePublished" datetime="2020-03-25T15:27:57+08:00">2020-03-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>下述的大多数方法来自 <strong><em><a href="https://arxiv.org/pdf/1503.03832.pdf" target="_blank" rel="noopener">FaceNet</a></em></strong>,也叫做<strong><em>DeepFace</em></strong>.</p>
<h2 id="脸部识别"><a href="#脸部识别" class="headerlink" title="脸部识别"></a>脸部识别</h2><p>一般来说，脸部识别问题可以分为两类：</p>
<ul>
<li>脸部验证(Face Verification)：”这是某个人吗？” 通过提供的输入的数据来识别是否是某个确定的人。比如机场系统扫描你的护照来确认你是否是正确的持有人，比如移动手机通过识别你的脸部确认你是拥有者从而解锁。总的来说，这是一个1对1匹配的问题。</li>
<li>脸部识别(Face Recognition)：”这是谁？” 通过提供的输入识别来识别对应的人是谁。比如公司的脸部识别打卡，通过识别脸部直接完成对应人员的打卡。总的来说，这是一个1对K的匹配问题。</li>
</ul>
<p>FaceNet 通过一个神经网络先将输入的脸部照片解码为一个128维向量，通过比较2个这样的向量，从而判断这两张图片是否是一个人。将输入数据与数据库中所有人员的照片进行对比，从而找到”这是谁”。</p>
<p>我们将用TensorFlow来实现这个程序：(tensorflow 1.X)</p>
<ul>
<li>1.实现三元损失函数</li>
<li>2.用一个预训练的模型来将脸部图片解码为128维的向量</li>
<li>3.使用这些代码来进行脸部验证和脸部识别</li>
</ul>
<p>另外，我们使用通道优先(channels-first)。 也就是说对于输入数据的维度表示，我们使用(m,nC,nH,nW), 而不是(m,nH,nW,nC)。当然，channels-first 和 channels-last都有各自的理由，至今社区也没有一个统一的标准。</p>
<h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><p>先导入包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">from keras.models import Sequential</span><br><span class="line">from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate</span><br><span class="line">from keras.models import Model</span><br><span class="line">from keras.layers.normalization import BatchNormalization</span><br><span class="line">from keras.layers.pooling import MaxPooling2D, AveragePooling2D</span><br><span class="line">from keras.layers.merge import Concatenate</span><br><span class="line">from keras.layers.core import Lambda, Flatten, Dense</span><br><span class="line">from keras.initializers import glorot_uniform</span><br><span class="line">from keras.engine.topology import Layer</span><br><span class="line">from keras import backend as K</span><br><span class="line">K.set_image_data_format(&#39;channels_first&#39;)</span><br><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">from numpy import genfromtxt</span><br><span class="line">import pandas as pd</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from fr_utils import *</span><br><span class="line">from inception_blocks_v2 import *</span><br><span class="line"></span><br><span class="line">np.set_printoptions(threshold&#x3D;np.nan)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="0-简陋的脸部识别"><a href="#0-简陋的脸部识别" class="headerlink" title="0.简陋的脸部识别"></a>0.简陋的脸部识别</h2><p>在脸部验证中，你需要确定给到的两张图片是否是一个人。最简单的方法就是直接将两张图片一个像素一个像素的进行比较。如果两张图片间的间距小于某个阈值，他们可能就是一个人。</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/LisaFaceReco.png" class="" title="逐像素比较">

<p>不难想到这个算法的表现会很差。因为光线的变化、人物脸部方向、甚至是头部位置的微小变化，都会导致像素值的改变。</p>
<p>与其使用原图片，我们可以使用编码后的图片数据。即f(img)。</p>
<p>将图片编码后的数据进行元素级的比较，我们可以得到一个更加准确的关于脸部验证的结果。</p>
<hr>
<h2 id="1-将脸部图片编码为128维的向量"><a href="#1-将脸部图片编码为128维的向量" class="headerlink" title="1.将脸部图片编码为128维的向量"></a>1.将脸部图片编码为128维的向量</h2><h3 id="1-1-使用卷积网络来进行编码"><a href="#1-1-使用卷积网络来进行编码" class="headerlink" title="1.1 使用卷积网络来进行编码"></a>1.1 使用卷积网络来进行编码</h3><p>FaceNet模型需要使用非常多的数据和很长的时间来进行训练。这里我们跳过这个步骤，直接载入别人已经训练好的权重。 网络结构采用了 <a href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener">Szegedy等人</a>文中的inception模型。 我们使用一个已经实现好的inception network的实现(在inception_blocks_v2.py中，略)。</p>
<p>几个需要知道的知识点：</p>
<ul>
<li>这个网络采用96×96维度的RGB图像作为输入。特别地，输入一个脸部照片(或者多批次的m照片组)作为张量，形状为：(m,nC,nH,nW) = (m,3,96,96)</li>
<li>它将输出一个(m,128)的矩阵，即将每一张图片都编码为128维的向量。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FRmodel &#x3D; faceRecoModel(input_shape&#x3D;(3, 96, 96))</span><br><span class="line">print(&quot;Total Params:&quot;, FRmodel.count_params())</span><br></pre></td></tr></table></figure>
<p>输出：Total Params: 3743280</p>
<p>通过使用一个128元的全连接层作为它的最后一层，这就保证了模型将输出128维的向量。接着，使用这两个向量进行两张图片的比较：</p>


<p>如果编码符合以下判别标准，则是一个好的编码:</p>
<ul>
<li>对同一个人的不同图片的编码较为相似</li>
<li>对不同人的图片的编码差异较大</li>
</ul>
<p>三元损失函数将以上标准公式化了，并且试图将相同人的图片的编码缩小，将不同人图片的编码拉大。</p>
<h3 id="1-2-三元损失函数对于一张图片x，我们声明它的编码为f-x-f是由神经网络计算的方法。"><a href="#1-2-三元损失函数对于一张图片x，我们声明它的编码为f-x-f是由神经网络计算的方法。" class="headerlink" title="1.2 三元损失函数对于一张图片x，我们声明它的编码为f(x), f是由神经网络计算的方法。"></a>1.2 三元损失函数对于一张图片x，我们声明它的编码为f(x), f是由神经网络计算的方法。</h3><img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/FaceNetFx.png" class="" title="f(x)方法">

<p>三元损失函数的训练需要用到三元组数据，每个三元组包含三张图片(A,P,N)</p>
<ul>
<li>A 是一张锚图片 - 某人的头部图像</li>
<li>P 是一张”正”图片 - 与A图片中是同一个人物</li>
<li>N 是一张”负”图片 - 与A图片中不是同一个人物</li>
</ul>
<p>我们用(A(i),P(i),N(i))(都是上标)来声明第i个训练样本。<br>我们希望图A(i)与P(i)的距离至少比A(i)和N(i)的距离近至少一个α的值：</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/lossFormulaSingle.png" class="" title="单个样本的三元损失公式">
<p>那么总的损失函数就是：</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/lossFormulaTotal.png" class="" title="三元损失公式">
<p>[z]+表示 max(z,0)。表示一旦A与P距离和A与N距离达到我们的要求，损失就为0，否则它的值就是损失值。</p>
<p><strong><em>Notes:</em></strong></p>
<ul>
<li>公式中的第一个部分是锚图片A和正图片P的距离，你希望它尽可能的小</li>
<li>公式的第二个部分则是锚图片A和负图片N的距离，你希望它相对较大</li>
<li>α叫做边距(margin)，这是一个人为选择的超参数，我们使用 α = 0.2</li>
</ul>
<p>大多数的实现里会将编码后的向量进行一次L2归一化，这里我们不用担心~</p>
<p>实现上面公式中的三元损失函数，需要4个步骤：</p>
<ol>
<li>计算锚图片A和正图片P间的距离</li>
<li>计算锚图片A和负图片N间的距离</li>
<li>对每个三元组样本进行公式计算</li>
<li>将每组样本经步骤3得到的值与0取max并取和</li>
</ol>
<p>PS：</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/L2formula.png" class="" title="L2 Norm计算方法">

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def triplet_loss(y_true, y_pred, alpha &#x3D; 0.2):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    三元损失函数的实现</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    y_true -- true 标签, 当你在keras中定义loss时需要, 在这个方法中你不需要它.</span><br><span class="line">    y_pred -- python list 包含三个对象:</span><br><span class="line">            anchor -- 锚图片编码后的结果, 形状为 (None, 128)</span><br><span class="line">            positive -- 正图片编码后的结果, 形状为(None, 128)</span><br><span class="line">            negative -- 负图片编码后的结果, 形状为 (None, 128)</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    loss -- 数字, 损失值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    anchor, positive, negative &#x3D; y_pred[0], y_pred[1], y_pred[2]</span><br><span class="line">    </span><br><span class="line">    # Step 1</span><br><span class="line">    pos_dist &#x3D; tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),-1)</span><br><span class="line">    # Step 2</span><br><span class="line">    neg_dist &#x3D; tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),-1)</span><br><span class="line">    # Step 3</span><br><span class="line">    basic_loss &#x3D; tf.maximum(tf.add(tf.subtract(pos_dist,neg_dist),alpha),0)</span><br><span class="line">    # Step 4</span><br><span class="line">    loss &#x3D; tf.reduce_sum(basic_loss)</span><br><span class="line">    </span><br><span class="line">    return loss</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="2-载入预训练模型"><a href="#2-载入预训练模型" class="headerlink" title="2.载入预训练模型"></a>2.载入预训练模型</h2><p>FaceNet通过最小化三元损失函数来进行训练。但训练需要大量的数据和计算时间，这里我们就不从头训练了。我们直接读取一个预训练的模型。用下面的代码读取来读取一个模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FRmodel.compile(optimizer &#x3D; &#39;adam&#39;, loss &#x3D; triplet_loss, metrics &#x3D; [&#39;accuracy&#39;])</span><br><span class="line">load_weights_from_FaceNet(FRmodel)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="3-应用模型"><a href="#3-应用模型" class="headerlink" title="3.应用模型"></a>3.应用模型</h2><p>假定我们构建的这个系统是一个门禁系统，用于给某公司利用脸部识别来确定是否允许某人进入公司建筑。</p>
<p>要通过门禁，每个人要先在入口处刷门禁卡，脸部识别系统会识别他们是否是他们所声明的人。</p>
<h3 id="3-1-脸部识别"><a href="#3-1-脸部识别" class="headerlink" title="3.1 脸部识别"></a>3.1 脸部识别</h3><p>我们先建立一个数据库，它存放了所有被允许进入建筑人员的编码后向量数据。它将用到img_to_encoding(image_path,model)方法，这个方法在输入图片数据上通过模型的前向传播来获得结果。</p>
<p>因为是教程，简便起见，我们直接用一个dict来充当数据库：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">database &#x3D; &#123;&#125;</span><br><span class="line">database[&quot;danielle&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;danielle.png&quot;, FRmodel)</span><br><span class="line">database[&quot;younes&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;younes.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;tian&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;tian.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;andrew&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;andrew.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;kian&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;kian.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;dan&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;dan.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;sebastiano&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;sebastiano.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;bertrand&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;bertrand.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;kevin&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;kevin.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;felix&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;felix.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;benoit&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;benoit.jpg&quot;, FRmodel)</span><br><span class="line">database[&quot;arnaud&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;arnaud.jpg&quot;, FRmodel)</span><br></pre></td></tr></table></figure>

<p>接下来，当一个人走到前门处并刷卡，你就可以从数据库中查找他的编码，然后再进行脸部匹配，主要以下几个步骤：</p>
<ol>
<li>将前门摄像机捕捉的图片进行编码</li>
<li>计算上一步的编码与数据库中找到的对应id人员的编码间的间距</li>
<li>如果间距小于0.7，开门</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">def verify(image_path, identity, database, model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    验证存放在image_path中的人是否和数据库identity对应的人是同一个</span><br><span class="line">    </span><br><span class="line">    参数:</span><br><span class="line">    image_path -- 图片地址</span><br><span class="line">    identity -- string, 要识别者的名字(来自于刷卡id). 必须是建筑进入允许的人员.</span><br><span class="line">    database -- python dictionary 数据字典 人名:头像编码 (向量).</span><br><span class="line">    model -- keras的 inception 模型</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    dist -- image_path存储的图像和identity对应的图像的间距</span><br><span class="line">    door_open -- True代表开门，False代表不开门</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    # Step 1:</span><br><span class="line">    encoding &#x3D; img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    # Step 2: </span><br><span class="line">    dist &#x3D; np.linalg.norm(encoding - database[identity])</span><br><span class="line">    # Step 3: </span><br><span class="line">    if dist &lt; 0.7:</span><br><span class="line">        print(&quot;It&#39;s &quot; + str(identity) + &quot;, welcome in!&quot;)</span><br><span class="line">        door_open &#x3D; True</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;It&#39;s not &quot; + str(identity) + &quot;, please go away&quot;)</span><br><span class="line">        door_open &#x3D; False</span><br><span class="line">        </span><br><span class="line">    return dist, door_open</span><br></pre></td></tr></table></figure>
<p>用了 <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html" target="_blank" rel="noopener">np.linalg.norm</a>来计算间距，不传递第二个参数即计算F-范数。</p>
<p>我们传入一张正确的图片试一试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">verify(&quot;images&#x2F;camera_0.jpg&quot;, &quot;younes&quot;, database, FRmodel)</span><br></pre></td></tr></table></figure>
<p>输出：It’s younes, welcome in!</p>
<p>再来一张错误的呢：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">verify(&quot;images&#x2F;camera_2.jpg&quot;, &quot;kian&quot;, database, FRmodel)</span><br></pre></td></tr></table></figure>
<p>输出：It’s not kian, please go away</p>
<h3 id="3-2-脸部识别"><a href="#3-2-脸部识别" class="headerlink" title="3.2 脸部识别"></a>3.2 脸部识别</h3><p>脸部认证系统基本完成了，但是如果系统内某人丢失了ID卡，他再次回到办公室就不能再进去了！(需要刷卡)</p>
<p>要解决这个问题，你就需要将系统改造成一个脸部识别系统。这样大家就都不需要带id卡了。一个被授权的人只要走到前门，门就会自动打开！</p>
<p>很简单，只需要一个遍历，直接上代码即可：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def who_is_it(image_path, database, model):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    实现脸部识别系统，识别image_path图片人的身份</span><br><span class="line">    </span><br><span class="line">    Arguments:</span><br><span class="line">    略</span><br><span class="line">    </span><br><span class="line">    Returns:</span><br><span class="line">    min_dist -- image_path图片与数据库中图片的最小间距</span><br><span class="line">    identity -- 最小间距对应的人</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    </span><br><span class="line">    encoding &#x3D; img_to_encoding(image_path,model)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    # 初始化最小值，整大点</span><br><span class="line">    min_dist &#x3D; 100</span><br><span class="line">    </span><br><span class="line">    for (name, db_enc) in database.items():</span><br><span class="line">        </span><br><span class="line">        dist &#x3D; np.linalg.norm(encoding - db_enc)</span><br><span class="line"></span><br><span class="line">        if dist &lt; min_dist:</span><br><span class="line">            min_dist &#x3D; dist</span><br><span class="line">            identity &#x3D; name</span><br><span class="line"></span><br><span class="line">    if min_dist &gt; 0.7:</span><br><span class="line">        print(&quot;Not in the database.&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print (&quot;it&#39;s &quot; + str(identity) + &quot;, the distance is &quot; + str(min_dist))</span><br><span class="line">        </span><br><span class="line">    return min_dist, identity</span><br></pre></td></tr></table></figure>
<p>试一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">who_is_it(&quot;images&#x2F;camera_0.jpg&quot;, database, FRmodel)</span><br></pre></td></tr></table></figure>
<p>输出：it’s younes, the distance is 0.659393</p>
<p>激动人心的时候来了，我们把Lisa的图片裁剪成96×96再放入：</p>
<img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/Lisa.png" class="" title="Lisa"> <img src="/2020/03/25/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F/camera_Lisa.png" class="" title="LisaInCamera">
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">database[&quot;lisa&quot;] &#x3D; img_to_encoding(&quot;images&#x2F;Lisa.png&quot;, FRmodel)</span><br><span class="line">who_is_it(&quot;images&#x2F;camera_Lisa.png&quot;, database, FRmodel)</span><br></pre></td></tr></table></figure>
<p>输出： it’s lisa, the distance is 0.597898<br>成功！！！</p>
<p>这样，一个简陋版本的脸部识别系统就完成啦！</p>
<h3 id="一些提升的方法"><a href="#一些提升的方法" class="headerlink" title="一些提升的方法"></a>一些提升的方法</h3><p>这里就不一一实现了，还有一些可以提升算法效果的方法：</p>
<ul>
<li>对于每个人，多在数据库中放几张照片，比如不同角度的，不同光线的，不同时间的。在刷脸时，将之与数据库中每个人的多张图片进行比较，这样可以提高模型准确度。</li>
<li>运用一个裁剪算法，将图片尽量剪到只剩下脸部。这样可以尽量排除不相关因素的干扰，也能提高准确度。</li>
</ul>
<hr>
<h2 id="引用："><a href="#引用：" class="headerlink" title="引用："></a>引用：</h2><ul>
<li>Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). <a href="https://arxiv.org/pdf/1503.03832.pdf" target="_blank" rel="noopener">FaceNet: A Unified Embedding for Face Recognition and Clustering</a></li>
<li>Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf (2014). <a href="https://research.fb.com/wp-content/uploads/2016/11/deepface-closing-the-gap-to-human-level-performance-in-face-verification.pdf" target="_blank" rel="noopener">DeepFace: Closing the gap to human-level performance in face verification</a></li>
<li>The pretrained model we use is inspired by Victor Sy Wang’s implementation and was loaded using his code: <a href="https://github.com/iwantooxxoox/Keras-OpenFace" target="_blank" rel="noopener">https://github.com/iwantooxxoox/Keras-OpenFace</a>.</li>
<li>Our implementation also took a lot of inspiration from the official FaceNet github repository: <a href="https://github.com/davidsandberg/facenet" target="_blank" rel="noopener">https://github.com/davidsandberg/facenet</a></li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://toulondu.github.io/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Toulon Du">
      <meta itemprop="description" content="Sharing Knowledge And Learn More">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Toulon's BLOG">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/" class="post-title-link" itemprop="url">卷积神经网络介绍</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-03-24 15:07:43" itemprop="dateCreated datePublished" datetime="2020-03-24T15:07:43+08:00">2020-03-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-03-26 21:22:36" itemprop="dateModified" datetime="2020-03-26T21:22:36+08:00">2020-03-26</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>传统的神经网络的全连接层(full connected)在遭遇较大的输入数据时，需要训练的参数将会非常多。举个例子，一张1000×1000的图片，作flatten处理后就有了1000×1000×3的输入维度，如果第二层的隐藏单元数是1000，参数W的维度就是3000000×1000。<br>参数过多，就意味着训练难度变高。在这种情况下，卷积神经网络就诞生了。</p>
<h2 id="什么是卷积"><a href="#什么是卷积" class="headerlink" title="什么是卷积"></a>什么是卷积</h2><p>我们先来看一下最基础的一点：什么是卷积？</p>
<p>用一个例子来进行说明，假如我们有一张6×6的图片，为了方便理解，我们暂时假定它第三个维度为1，即通道数为1。那么一次卷积操作如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/conv01.png" class="" title="卷积1">

<p>如上图所示，左一的6×6矩阵代表我们的输入矩阵，中间的3×3矩阵叫做过滤器(filter)，也叫做卷积核，而二者中间的”*“就表示<strong><em>卷积</em></strong>，它和计算机中的乘法符号一致。</p>
<p>卷积的运算方法就是 从输入矩阵的左上角开始，从左到右每一次取出一个与卷积核维度一样大小的矩阵，这里是3×3。 将取出的矩阵的每一位与卷积核对应为相乘并相加，得到的数据填在右侧结果矩阵的对应位置。</p>
<p>第二步如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/conv02.png" class="" title="卷积2">

<p>以此类推，往右不能再走时，就向下移动一行，再次从最左边开始取。<br>这样，最终我们可以得到一个4×4的结果矩阵(一张变小的图片)。</p>
<h3 id="卷积在做什么"><a href="#卷积在做什么" class="headerlink" title="卷积在做什么"></a>卷积在做什么</h3><p>你可能会感到困惑，这样的一步所谓的操作到底做了些什么。<br>就我们上面的例子来说，这样一个卷积核的效果可以用六个字来概括：“垂直边缘检测”。<br>如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/conv03.png" class="" title="卷积3">
<p>可以看到，结果图片的中部有一道明显的白色，这就是检测出来的垂直边缘。是的，它看起来与左边的输入图片有些不符合，显得比较厚，这是因为我们选择的图片较小(才6×6)，当把图片变大时，结果就变得较为可观了。</p>
<p>既然有垂直边缘检测的卷积核，自然就有其它功能的卷积核，比如水平边缘检测卷积核，比如着重突出某些边缘特征的卷积核等。</p>
<p>而卷积核的值是可以通过训练得到的，这点我们将在后面介绍。</p>
<h2 id="填充-padding"><a href="#填充-padding" class="headerlink" title="填充-padding"></a>填充-padding</h2><p>除了卷积，conv网络的第二个重要的积木就是<strong><em>填充</em></strong>。</p>
<p>在上一节介绍卷积时，一个6×6的图片经过一次卷积操作变成了4×4，可见卷积操作是会让输入数据的维度降低的。这样的操作多来几次，再大的图片也遭不住。因此，为了构建深层网络，填充就必不可少。</p>
<p>首先，关于卷积操作减小维度，是有一个通用的计算公式的：<br><strong><em>结果矩阵的维度 = (n-f+1) × (n-f+1)</em></strong><br>其中f为卷积核的维度，n为输入矩阵的维度。</p>
<p>除了会减小图片的维度外，卷积还会导致图片角落的像素只能被使用到一次，而中间的数据则会被使用多次，这样会导致图片边缘的数据不能得到足够的利用，甚至被丢失了。</p>
<p>所以所谓填充，就是填充数据的边缘：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/padding01.png" class="" title="填充1">
<p>通过在原图片边缘填充一层使之变成8×8的图像，从而经过卷积操作后，你依然可以得到一个6×6的图像。</p>
<p>一般来说，在卷积网络中大多数时候使用这样的填充，即使数据在填充后进行卷积得到与原数据同样维度的数据。这样的卷积操作我们称之为<strong><em>same卷积</em></strong>。</p>
<p>除此之外还有full卷积和valid卷积，前者让图像最角落的元素也可以被充分利用，而后者则是不做填充。</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/padding02.png" class="" title="full卷积">
<p>上图为full卷积</p>
<h2 id="步幅-stride-卷积"><a href="#步幅-stride-卷积" class="headerlink" title="步幅(stride)卷积"></a>步幅(stride)卷积</h2><p>在之前的介绍中，我们每次计算卷积后，都是在原图片上向右或者向下移动一步。而步幅卷积，顾名思义，就是将这个一步扩展开来，每次移动s步，我们以2为例：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/stride01.png" class="" title="步幅卷积1">

<p>如上为第一步和第二步，此外，当需要向下移动时，也是移动s步</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/stride03.png" class="" title="步幅卷积3">

<p>容易计算，经过步幅为s的卷积后，结果矩阵维度的计算方式为：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/strideFormula.png" class="" title="步幅卷积结果矩阵维度公式">

<p>假如当向右移动s步已经超出了原矩阵范围，即当不能被整除的时候，就直接舍弃剩余部分直接向下走了。即向下取整。</p>
<h2 id="三维数据的卷积"><a href="#三维数据的卷积" class="headerlink" title="三维数据的卷积"></a>三维数据的卷积</h2><p>对于现实情况中的图片来说，因为RGB格式的存在，我们的输入数据大多都是3维的，那么，如何将卷积扩展到三维数据上去呢？</p>
<p>首先，卷积核（过滤器）也要变成三维，且第三个维度要和原矩阵相同(第三个维度在卷积中我们一般叫做<strong><em>通道</em></strong>)，如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/convIn3D01.png" class="" title="三维数据卷积01">
<p>可以看到，最终的结果是一个4*4的矩阵，没有了第三个维度，计算方法其实很简单，基本就是二维计算方式的三维扩展：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/convIn3D02.png" class="" title="三维数据卷积02">
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/convIn3D03.png" class="" title="三维数据卷积03">
<p>即把每个3×3×3 = 27个格子的数字对应相乘并相加，然后右移和下移即可，因为深度一致，并不需要关心第三个维度。</p>
<p>卷积核的设计很灵活，假如你把卷积核的第二层和第三层都设置为0，相当于在结果矩阵中就只有你对第一层的卷积，在RGB图片中，你就只检测了红色通道的边缘。</p>
<p>更常用的是，你可以同时使用多个卷积核，然后把得到的结果堆叠，得到通道数大于1的结果：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/convIn3D04.png" class="" title="三维数据卷积04">
<p><strong>所以卷积结果矩阵的通道数，取决于卷积核的数量，记住这一点。</strong></p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>介绍了这么多，我们拉通来看一下，把上述知识点结合起来，就是卷积神经网络的一层：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/aLayerInConvNet.png" class="" title="卷及网络中的一层">
<p>之前我们说过，每一个过滤器(卷积核)都相当于是过滤出原数据的一个特性。上图中，用了2个过滤器，将每个过滤器得到的结果再进行一次激活。激活函数可以说是神经网络中的一种重要工具，一般是一个非线性函数，可以简单地理解为将数据中的特点放大并且保证了多层网络的实用性。激活之后再重叠在一起，就得到了我们最后4×4×2的结果。</p>
<p>如果你知道普通神经网络的结构，它每一层的运算就是先进行 z[l] = W*a[l-1] + b[l]，再进行 a[l] = g(z[l])。可以看到对于我们的卷积神经网络，这一层的操作，也可以对应成这个公式，过滤器就是我们的W。</p>
<p>既然过滤器是我们W，上面我们也说过过滤器的值可以训练，那么这么一层卷积网络，我们需要训练多少参数呢？<br>假设我们有10个3×3×3的过滤器，那么我们需要 3×3×3×10 + 10(b参数，偏差值) 个，即280个参数。</p>
<h3 id="ConvNet中的符号"><a href="#ConvNet中的符号" class="headerlink" title="ConvNet中的符号"></a>ConvNet中的符号</h3><p>在卷积网络中，你会看到很多各种符号以及参数、中间值，结果的维度，我们稍作总结：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/notaionInConvNet.png" class="" title="卷积网络中的符号">

<p>以上就是卷积网络中的一层了，那么我们如何扩展它来构建一个卷积网络呢？</p>
<p>PS: ConvNet是卷积神经网络的简写</p>
<h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h3><p>现在假如你有一些照片，你想通过卷积网络来实现图片的分类，假如输入图片的大小为39×39×3：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/aBriefConvNet.png" class="" title="一个简单的卷积网络结构">
<p>接着，我们可以把最后这个7×7×40的矩阵展开成1960的向量，接着我们可以使用普通神经网络的手段，使用逻辑回归或者一个softmax层来进行最后结果的计算。</p>
<p>从上面可以看出一个大多数卷积神经网络的趋势，即随着层数的增加，矩阵的宽高会逐渐减小，而通道数量会逐渐增加。</p>
<h3 id="其它的层类型"><a href="#其它的层类型" class="headerlink" title="其它的层类型"></a>其它的层类型</h3><p>上面的网络中，这些使用卷积核的层都可以叫做卷积层，但神经网络层中还有一些其他的层类型，比如池化(pooling)层，全连接(full connect/FC)层等。<br>将卷积层与这二者结合，我们可以搭建出更好的卷积神经网络。</p>
<h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>在ConvNet中，经常会使用一种叫做池化层(pooling layer)的结构来减小表示层的大小，从而加快模型训练，甚至更好的发现一些特性。</p>
<p>下面介绍两种池化层</p>
<h3 id="最大值-Max-池化"><a href="#最大值-Max-池化" class="headerlink" title="最大值(Max)池化"></a>最大值(Max)池化</h3><p>非常简单，如下：</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/maxPooling.png" class="" title="最大值池化">
<p>虽然很简单，但max池化在很多的实际应用中都有很好的效果。为什么呢？<br>直白的理解一下，加入你把这个4×4的区域看作某个特征的集合，即神经网络某个层中的激活状态，那么一个大的数字意味着算法可能检测到了一个特定的特征，如上图左上的2*2区域就有这样的特征，8被挑选并保留下来，它可能是一个垂直边缘，可能是一个任何难以描述的特征，而右上区域没有这样的特征，它的最大值依然很小。<br>所以，max pooling作所做的其实是 ，如果在过滤器中任何地方检测到了任何特征，就保留最大的数值。<br>反之，如果没有明显的特征被检测到，比如右上方的四分之一区域就没有这个特征，在结果中那些数值的最大值仍然相当小。</p>
<h3 id="平均值池化"><a href="#平均值池化" class="headerlink" title="平均值池化"></a>平均值池化</h3><img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/avgPooling.png" class="" title="平均值池化">
<p>就不介绍了…</p>
<h3 id="池化层特点"><a href="#池化层特点" class="headerlink" title="池化层特点"></a>池化层特点</h3><p>最大值池化的使用通常比均值池化多得多。<br>也许你注意到了，大多数池化层都不需要学习超参数，只要确定了f和s，计算就确定了。<br>而且如果输入数据是3维的，则结果中通道数量依然保持，即每一层都像第一层一样进行过滤。</p>
<h2 id="卷积网络样例"><a href="#卷积网络样例" class="headerlink" title="卷积网络样例"></a>卷积网络样例</h2><p>todo…</p>
<h2 id="卷积为什么有用"><a href="#卷积为什么有用" class="headerlink" title="卷积为什么有用"></a>卷积为什么有用</h2><p>比起一般的全连接神经网络，卷积神经网络主要有2个优势，参数共享和连接稀疏性。</p>
<img src="/2020/03/24/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%BB%8B%E7%BB%8D/whyCcnvWork.png" class="" title="例图"> 
<p>举个例子，如上图，第一层共32×32×3 = 3072个输入，第二层共4704个输出。使用如上所示的卷积，我们只需要(5×5×3+1)×6一共456个参数。而如果我们使用全连接层，我们需要多少参数呢？一共3072×4704个参数，多太多了。</p>
<p>卷积网络的参数少，主要是两个原因。</p>
<h3 id="参数共享"><a href="#参数共享" class="headerlink" title="参数共享"></a>参数共享</h3><p>每一次卷积的时候，我们的卷积核一直被轮流地使用，输入数据的每一个部分都与过滤器进行了计算。<br>所谓共享即卷积核的共享。</p>
<h3 id="连接的稀疏性"><a href="#连接的稀疏性" class="headerlink" title="连接的稀疏性"></a>连接的稀疏性</h3><p>也很好理解，比如看上图，卷积后结果矩阵的每一个值，都只是原输入数据的一个3*3的数据与过滤器发生计算得到的，不像全连接要关联所有输入值。</p>
<p>更多，也有人说卷积可以捕捉平移不变，即是说一张图片的像素右移或左移了一些像素，它们依然应该有相同的特征。卷积可以给他们打上相同的标签。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这就是卷积神经网络的基本概念，通过tensorflow等软件包你也可以很轻松的实现各种自定义的网络类型。但一般的实际应用中，并不推荐自由构建网络结构，因为卷积网络的训练时间普遍较长，所以当你要建立一个卷积网络来解决一个新的问题时，去尝试想当然的网络结构往往得不偿失。一个好的方法就是多从各大论文或者成功模型的分享中获得灵感，在别人推荐的模型和超参数的基础上构建你的网络，才能事半功倍。</p>
<p>之后会介绍一些经典的卷积网络模型。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Toulon Du</p>
  <div class="site-description" itemprop="description">Sharing Knowledge And Learn More</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Toulon Du</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
